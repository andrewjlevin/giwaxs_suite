{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c13da0-ce45-4653-9fb2-5f71078d5a36",
   "metadata": {},
   "source": [
    "# CMS GIWAXS plotting notebook - plotting single images from loaded zarr datasets\n",
    "# PM6 Y6-series solvent study GIWAXS CMS 2024C2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96625ca6-7ec2-4690-bf01-72b422801f76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd667c0e-baba-4a5d-857a-ca8bd5ce1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pathlib\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Choose a colormap:\n",
    "cmap = plt.cm.turbo\n",
    "cmap.set_bad('black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4dc718-68e9-4a1c-a13b-b8d7c54d88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize, signal\n",
    "from lmfit import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffa6de-0360-4fcb-b0bf-f320927837d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define & check paths"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a200b152-8c3b-428f-8b58-25cb3e6f270a",
   "metadata": {},
   "source": [
    "rclone --dry-run copy -P /nsls2/data/cms/proposals/2023-3/pass-311415/AL_2024C2/processed_data/giwaxs_plots remote:research/data_analysis/giwaxs_suite/processed_data/2024C2_cms\n",
    "\n",
    "rclone --dry-run copy -P /nsls2/data/cms/proposals/2023-3/pass-311415/AL_2024C2/processed_data/full_linefits_v2 remote:research/data_analysis/giwaxs_suite/processed_data/2024C2_cms/full_linefits_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63ad6f-1b77-4925-9708-38f4fd8ccefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_attrs(data_arrays_iterable, selected_attrs_dict):\n",
    "    \"\"\"\n",
    "    Selects data arrays whose attributes match the specified values.\n",
    "\n",
    "    Parameters:\n",
    "    data_arrays_iterable: Iterable of xarray.DataArray objects.\n",
    "    selected_attrs_dict: Dictionary where keys are attribute names and \n",
    "                         values are the attributes' desired values.\n",
    "\n",
    "    Returns:\n",
    "    List of xarray.DataArray objects that match the specified attributes.\n",
    "    \"\"\"    \n",
    "    sublist = list(data_arrays_iterable)\n",
    "    \n",
    "    for attr_name, attr_values in selected_attrs_dict.items():\n",
    "        sublist = [da.copy() for da in sublist if da.attrs[attr_name] in attr_values]\n",
    "                \n",
    "    return sublist\n",
    "\n",
    "def fold_image(data_array, fold_axis, progress_bar=False):\n",
    "    \"\"\"\n",
    "    Method to fold image along a specified axis.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_array (xarray DataArray): The DataArray to fold\n",
    "    - fold_axis (str): The axis along which to fold the image\n",
    "    \n",
    "    Returns:\n",
    "    - xarray DataArray: The folded image\n",
    "    \"\"\"\n",
    "    # Filter data for fold_axis >= 0 and fold_axis <= 0\n",
    "    positive_data = data_array.where(data_array[fold_axis] >= 0, drop=True)\n",
    "    negative_data = data_array.where(data_array[fold_axis] <= 0, drop=True)\n",
    "    \n",
    "    # Reverse negative_data for easier comparison\n",
    "    negative_data = negative_data.reindex({fold_axis: negative_data[fold_axis][::-1]})\n",
    "    \n",
    "    # Find the maximum coordinate of the shorter quadrant (positive_data)\n",
    "    max_positive_coord = float(positive_data[fold_axis].max())\n",
    "    \n",
    "    # Find the equivalent coordinate in the negative_data\n",
    "    abs_diff = np.abs(negative_data[fold_axis].values + max_positive_coord)\n",
    "    \n",
    "    # Minimize the difference\n",
    "    min_diff_idx = np.argmin(abs_diff)\n",
    "    \n",
    "    # Check if the lengths are equivalent\n",
    "    len_pos = len(positive_data[fold_axis])\n",
    "    len_neg = len(negative_data[fold_axis][:min_diff_idx+1])\n",
    "    \n",
    "    if len_pos != len_neg:\n",
    "        # Adjust the coordinate range for negative_data\n",
    "        for i in range(1, 4):  # Check 3 neighbors\n",
    "            new_idx = min_diff_idx + i\n",
    "            len_neg = len(negative_data[fold_axis][:new_idx+1])\n",
    "            if len_pos == len_neg:\n",
    "                min_diff_idx = new_idx\n",
    "                break\n",
    "                \n",
    "    # Crop the negative_data to match positive_data length\n",
    "    negative_data_cropped = negative_data.isel({fold_axis: slice(0, min_diff_idx+1)})\n",
    "    \n",
    "    # Prepare the new data array\n",
    "    new_data = xr.zeros_like(positive_data)\n",
    "    \n",
    "    # Fold the image\n",
    "    if progress_bar:\n",
    "        for i in tqdm(range(len(positive_data[fold_axis]))):\n",
    "            pos_val = positive_data.isel({fold_axis: i}).values\n",
    "            neg_val = negative_data_cropped.isel({fold_axis: i}).values\n",
    "\n",
    "            # Pixel comparison and choosing \n",
    "            new_data[i] = np.where(((pos_val == 0) | (np.isnan(pos_val))) & (neg_val > 1-1e-6),\n",
    "                                   neg_val, \n",
    "                                   pos_val)\n",
    "    else:\n",
    "        for i in range(len(positive_data[fold_axis])):\n",
    "            pos_val = positive_data.isel({fold_axis: i}).values\n",
    "            neg_val = negative_data_cropped.isel({fold_axis: i}).values\n",
    "\n",
    "            # Pixel comparison and choosing \n",
    "            new_data[i] = np.where(((pos_val == 0) | (np.isnan(pos_val)) | (pos_val < 2)) & (neg_val > 0.001),\n",
    "                                   neg_val, \n",
    "                                   pos_val) \n",
    "            \n",
    "            # # Pixel comparison and averaging\n",
    "            # new_data[i] = np.where(\n",
    "            #     (pos_val > 0) & (neg_val > 0), \n",
    "            #     (pos_val + neg_val) / 2,\n",
    "            #     np.where(((pos_val == 0) | (np.isnan(pos_val))) & (neg_val > 0),\n",
    "            #              neg_val, \n",
    "            #              pos_val)\n",
    "            # )\n",
    "\n",
    "\n",
    "        \n",
    "    # Append residual data from the longer quadrant if exists\n",
    "    if len(negative_data[fold_axis]) > min_diff_idx+1:\n",
    "        residual_data = negative_data.isel({fold_axis: slice(min_diff_idx+1, None)})\n",
    "        residual_data[fold_axis] = np.abs(residual_data[fold_axis])\n",
    "        new_data = xr.concat([new_data, residual_data], dim=fold_axis)\n",
    "        \n",
    "    # Update data_array with the folded image\n",
    "    data_array = new_data.sortby(fold_axis)\n",
    "    \n",
    "    # Inherit coordinates and metadata attributes from the original data_array\n",
    "    data_array.attrs = data_array.attrs.copy()\n",
    "    data_array.attrs['fold_axis'] = fold_axis  # Add 'fold_axis' attribute\n",
    "\n",
    "    # Ensure all original coordinates are retained in the new data_array\n",
    "    for coord in data_array.coords:\n",
    "        if coord not in data_array.coords:\n",
    "            data_array = data_array.assign_coords({coord: data_array[coord]})\n",
    "\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0fc93-6739-457a-a7fe-ba695bb41716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like pathlib for its readability & checkability, it's also necessary for the loadSeries function later on\n",
    "# Replace the paths with the ones relevant to your data, you can use the \".exists()\" method to make sure you defined a path correctly\n",
    "userPath = pathlib.Path('/nsls2/users/alevin')\n",
    "propPath = pathlib.Path('/nsls2/data/cms/proposals/2023-3/pass-311415')\n",
    "zarrsPath = propPath.joinpath('AL_2024C2/processed_data/zarrs')\n",
    "outPath = propPath.joinpath('AL_2024C2/processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdac03-46b0-4814-8fed-6b0b3de72404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List zarrs\n",
    "sorted([f.name for f in zarrsPath.iterdir()])  # a way to list just the filenames and not the whole path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa803e77-7002-4dd4-a4e2-fbd95090784e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Corrected with flipped bar\n",
    "sn = {\n",
    "    \"AL01\": \"Y6\",\n",
    "    \"AL02\": \"Y6:PVK 1:1\",\n",
    "    \"AL03\": \"Y6:PVK 1:9\",\n",
    "    \"AL04\": \"A1\",\n",
    "    \"AL05\": \"A1:PVK 1:1\",\n",
    "    \"AL06\": \"A1:PVK 1:9\",\n",
    "    \"AL07\": \"A2\",\n",
    "    \"AL08\": \"A2:PVK 1:1\",\n",
    "    \"AL09\": \"A2:PVK 1:9\",\n",
    "    \"AL10\": \"A3\",\n",
    "    \"AL11\": \"A3:PVK 1:1\",\n",
    "    \"AL12\": \"A3:PVK 1:9\",\n",
    "    \"AL13\": \"Y6 CF:CB 4:1\",\n",
    "    \"AL14\": \"Y6 CF:CB 2:3\",\n",
    "    \"AL15\": \"Y6 CF:CB 2:3 + 0.5% CN\",\n",
    "    \"AL16\": \"PM6 CF:CB 4:1\",\n",
    "    \"AL17\": \"PM6 CF:CB 2:3\",\n",
    "    \"AL18\": \"PM6 CF:CB 2:3 + 0.5% CN\",\n",
    "    \"AL19\": \"PM6:Y6 CF:CB 4:1\",\n",
    "    \"AL20\": \"PM6:Y6 CF:CB 2:3\",\n",
    "    \"AL21\": \"PM6:Y6 CF:CB 2:3 + 0.5% CN\",\n",
    "    \"AL22\": \"PM6 CB\",\n",
    "    \"AL23\": \"Y6 CB\",\n",
    "    \"AL24\": \"Y6BO CB\",\n",
    "    \"AL25\": \"PM6 CB + 1% CN\",\n",
    "    \"AL26\": \"PM6 CB + 5% CN\",\n",
    "    \"AL27\": \"Y6 CB + 0.5% CN\",\n",
    "    \n",
    "    \"AL28\": \"PM6 CF + 1% CN\",\n",
    "    \"AL29\": \"Y6BO CF\",\n",
    "    \"AL30\": \"Y6 CF\",\n",
    "    \"AL31\": \"PM6 CF\",\n",
    "    \"AL32\": \"PM6:Y6BO CB + 0.5% CN\",\n",
    "    \"AL33\": \"PM6:Y6 CB + 0.5% CN\",\n",
    "    \"AL34\": \"PM6:Y6BO CB\",\n",
    "    \"AL35\": \"PM6:Y6 CB\",\n",
    "    \"AL36\": \"PM6 CB + 0.5% CN\",\n",
    "    \"AL37\": \"Y6BO CB + 0.5% CN\",\n",
    "    \n",
    "    \"AL38\": \"PM6 CF + 5% CN\",\n",
    "    \"AL39\": \"Y6 CF + 0.5% CN\",\n",
    "    \"AL40\": \"Y6BO CF + 0.5% CN\",\n",
    "    \"AL41\": \"PM6 CF + 0.5% CN\",\n",
    "    \"AL42\": \"PM6:Y6 CF\",\n",
    "    \"AL43\": \"PM6:Y6BO CF\",\n",
    "    \"AL44\": \"PM6:Y6 CF + 0.5% CN\",\n",
    "    \"AL45\": \"PM6:Y6BO CF + 0.5% CN\"\n",
    "}\n",
    "\n",
    "sn_id = {\n",
    "    \"AL01\": \"Y6-PVK_1-0\",\n",
    "    \"AL02\": \"Y6-PVK_1-1\",\n",
    "    \"AL03\": \"Y6-PVK_1-9\",\n",
    "    \"AL04\": \"A1-PVK_1-0\",\n",
    "    \"AL05\": \"A1-PVK_1-1\",\n",
    "    \"AL06\": \"A1-PVK_1-9\",\n",
    "    \"AL07\": \"A2-PVK_1-0\",\n",
    "    \"AL08\": \"A2-PVK_1-1\",\n",
    "    \"AL09\": \"A2-PVK_1-9\",\n",
    "    \"AL10\": \"A3-PVK_1-0\",\n",
    "    \"AL11\": \"A3-PVK_1-1\",\n",
    "    \"AL12\": \"A3-PVK_1-9\",\n",
    "    \"AL13\": \"Y6_4CF-1CB\",\n",
    "    \"AL14\": \"Y6_2CF-3CB\",\n",
    "    \"AL15\": \"Y6_p5CN-2CF-3CB\",\n",
    "    \"AL16\": \"PM6_4CF-1CB\",\n",
    "    \"AL17\": \"PM6_2CF-3CB\",\n",
    "    \"AL18\": \"PM6_p5CN-2CF-3CB\",\n",
    "    \"AL19\": \"PM6-Y6_4CF-1CB\",\n",
    "    \"AL20\": \"PM6-Y6_2CF-3CB\",\n",
    "    \"AL21\": \"PM6-Y6_p5CN-2CF-3CB\",\n",
    "    \"AL22\": \"PM6_0CN-CB\",\n",
    "    \"AL23\": \"Y6_CB\",\n",
    "    \"AL24\": \"Y6BO_CB\",\n",
    "    \"AL25\": \"PM6_1CN-CB\",\n",
    "    \"AL26\": \"PM6_5CN-CB\",\n",
    "    \"AL27\": \"Y6_p5CN-CB\",\n",
    "    \n",
    "    \n",
    "    \"AL28\": \"PM6_1CN-CF\",\n",
    "    \"AL29\": \"Y6BO_CF\",\n",
    "    \"AL30\": \"Y6_CF\",\n",
    "    \"AL31\": \"PM6_0CN-CF\",\n",
    "    \"AL32\": \"PM6-Y6BO_p5CN-CB\",\n",
    "    \"AL33\": \"PM6-Y6_p5CN-CB\",\n",
    "    \"AL34\": \"PM6-Y6BO_CB\",\n",
    "    \"AL35\": \"PM6-Y6_CB\",\n",
    "    \"AL36\": \"PM6_p5CN-CB\",\n",
    "    \"AL37\": \"Y6BO_p5CN-CB\",\n",
    "    \n",
    "    \"AL38\": \"PM6_5CN-CF\",    \n",
    "    \"AL39\": \"Y6_p5CN-CF\",\n",
    "    \"AL40\": \"Y6BO_p5CN-CF\",\n",
    "    \"AL41\": \"PM6_p5CN-CF\",\n",
    "    \"AL42\": \"PM6-Y6_CF\",\n",
    "    \"AL43\": \"PM6-Y6BO_CF\",\n",
    "    \"AL44\": \"PM6-Y6_p5CN-CF\",\n",
    "    \"AL45\": \"PM6-Y6BO_p5CN-CF\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd26a5-3875-47bb-a594-b1af830fa824",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Polar plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b333a-53cc-4ace-8769-f61a5a91f60c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638182f5-d3c2-40d2-bac5-f4e29107a1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[f.name for f in zarrsPath.glob('*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc28bc-15c1-4a00-8c65-cfcaa32e35d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'caked_DS.zarr'\n",
    "DS = xr.open_zarr(zarrsPath.joinpath(filename))\n",
    "DS = DS.where(DS>1e-6)\n",
    "DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ea1de-89fc-4c16-9002-46e8a3b05bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Apply a sin chi correction\n",
    "sin_chi_DA = np.sin(np.radians(np.abs(DS.chi)))\n",
    "\n",
    "corr_DS = DS.copy()\n",
    "# corr_DS = corr_DS * sin_chi_DA  # This works mathematically, but does not preserve attributes\n",
    "for var in corr_DS.data_vars:\n",
    "    corrected = corr_DS[var] * sin_chi_DA\n",
    "    corr_DS[var].values = corrected.values\n",
    "    \n",
    "corr_DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cadb73-aab5-474d-bf14-2fb724029824",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fold sin(chi) corrected dataset\n",
    "\n",
    "folded_corr_DAs = []\n",
    "for DA in tqdm(corr_DS.data_vars.values()):\n",
    "    folded_corr_DAs.append(fold_image(DA, 'chi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6806290-35a0-4b54-8fe7-2de99705ce7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "                                     'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "len(selected_DAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee6656-4c85-4ef6-a063-a31c2b922917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91281ce7-29ad-40ae-832b-af90460579ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PM6_DS = xr.Dataset()\n",
    "for DA in selected_DAs:\n",
    "    PM6_DS[DA.name] = DA.drop_encoding()\n",
    "    \n",
    "PM6_DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e6cfa-ec74-4109-90b0-42623e2dc964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "savePath = outPath.joinpath('giwaxs_plots/pm6_data')\n",
    "savePath.mkdir(exist_ok=True)\n",
    "PM6_DS.to_zarr(savePath.joinpath('PM6.zarr'), mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eef254-2427-4373-9cf6-ac17e09ee7d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2D caked images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d83d04-8ef7-4534-b845-480451693ff5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Folded sin(chi) intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4fd95-5ddd-44c6-ba4b-ff19903780f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35067fc8-27ec-4aa3-873c-66f7d080e86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Polar plots, for FOLDED sin(chi) intensities\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "# Set chi range: Full range\n",
    "chi_min = 0\n",
    "chi_max = 90\n",
    "q_min = 0.05\n",
    "q_max = 2.04\n",
    "\n",
    "# selected_attrs_dict = {}\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22'], 'incident_angle': ['th0.110']}\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL38']}\n",
    "\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)    \n",
    "for DA in tqdm(selected_DAs):\n",
    "    # Slice dataarray to select plotting region \n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min, q_max))\n",
    "    \n",
    "    # Set color limits\n",
    "    real_min = float(sliced_DA.compute().quantile(0.01))\n",
    "    cmin = 1 if real_min < 1 else real_min\n",
    "\n",
    "    cmax = float(sliced_DA.compute().quantile(0.995))       \n",
    "    \n",
    "    # Plot sliced dataarray\n",
    "    ax = sliced_DA.plot.imshow(origin='upper', cmap=cmap, norm=plt.Normalize(cmin, cmax), figsize=(5,4))  # plot, optional parameter interpolation='antialiased' for image smoothing\n",
    "    # ax.axes.set(title=f'Folded polar plot: {DA.material} {DA.solvent} {DA.rpm}, $\\\\alpha_i$ = {float(DA.incident_angle[2:])}°, sin($\\chi$) corr.')\n",
    "    ax.axes.set(title=f'Folded polar plot: {sn[DA.sample_ID]}, $\\\\alpha_i$ = {float(DA.incident_angle[2:])}°, sin($\\chi$) corr.')\n",
    "    ax.colorbar.set_label('Intensity * sin($\\chi$) [arb. units]', rotation=270, labelpad=15)  # set colorbar label & parameters \n",
    "    ax.axes.set(xlabel='q$_r$ [Å$^{-1}$]', ylabel='$\\chi$ [°]')  # set title, axis labels, misc\n",
    "    ax.figure.set(tight_layout=True, dpi=130)  # Adjust figure dpi & plotting style\n",
    "    \n",
    "    \n",
    "    # # Uncomment below line and set savepath/savename for saving plots, I usually like to check \n",
    "    # savePath = outPath.joinpath('giwaxs_plots/caked_plots_v1')\n",
    "    # savePath.mkdir(exist_ok=True)\n",
    "    # ax.figure.savefig(savePath.joinpath(f'sinchi-folded_{sn_id[DA.sample_ID]}_{DA.sample_pos}_chi{chi_min}to{chi_max}_q{q_min}to{q_max}_{DA.incident_angle}.png'), dpi=150)\n",
    "\n",
    "    plt.show()  # Comment to mute plotting output\n",
    "    # plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c5689-4a4e-4bb0-8c4e-48cbf0a8d252",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1D linecuts along chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbfa97e-4b25-47cd-a87b-2abf48860780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.close('all')\n",
    "\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "# chi_min, chi_width, chi_bins = [10, 6, 12]  \n",
    "chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "q_min = 0.18  # for full qr cut\n",
    "q_max = 1.9\n",
    "\n",
    "# Select attribute\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22', 'AL25', 'AL26'], 'incident_angle': ['th0.110']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "for DA in tqdm(selected_DAs):\n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "    binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5.5,3), dpi=150, tight_layout=True)\n",
    "    for i, chi_bin in enumerate(binned_DA.chi_bins.data[::-1]):\n",
    "        if i == 0 or i == 17:\n",
    "            binned_DA.sel(chi_bins=chi_bin).plot.line(ax=ax, color=colors[i], label=chi_bin)\n",
    "\n",
    "    ax.set(title=f'{sn[DA.sample_ID]} {DA.sample_pos} {DA.incident_angle}: $\\\\chi$-binned-summed linecuts:\\n'+ \n",
    "                 f'{chi_bins}, {chi_width}° $\\\\chi$ bins: from {int(chi_min)} to {int(chi_max)}°', \n",
    "           xlabel='$q_r$ $[Å^{-1}]$', \n",
    "           ylabel='$sin(\\\\chi) * Intensity$ [arb. units]')\n",
    "    ax.axes.xaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
    "    # ax.legend(title='Chi Bins')\n",
    "\n",
    "    # Create a colormap and normalizer to match the plotted data\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=chi_min, vmax=chi_max))\n",
    "    sm.set_array([])  # You need to set the array for the scalar mappable\n",
    "\n",
    "    # Create the colorbar\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='vertical')\n",
    "    cbar.set_label('Azimuthal Angle [°]', rotation=270, labelpad=15)\n",
    "    \n",
    "    # Define the ticks for the colorbar based on the chi bins\n",
    "    chi_values = np.linspace(chi_min, chi_max, int(chi_bins/2), endpoint=True)\n",
    "    cbar.set_ticks(chi_values)\n",
    "    cbar.set_ticklabels([f\"{value:.1f}\" for value in chi_values])\n",
    "\n",
    "    # # Save\n",
    "    # outPath.joinpath('chi-binned_linecuts_v1').mkdir(exist_ok=True)\n",
    "    # savePath = outPath.joinpath(\n",
    "    #     'chi-binned_linecuts_v1', f'full_chiWidth-{chi_width}_chiBins-{chi_bins}_chiRange{chi_min}-{chi_max}')\n",
    "    # # savePath = outPath.joinpath(\n",
    "    # #     'chi-binned_linecuts_v1', f'pipi_chiWidth-{chi_width}_chiBins-{chi_bins}_chiRange{chi_min}-{chi_max}')\n",
    "    # savePath.mkdir(exist_ok=True)\n",
    "    # ax.figure.savefig(savePath.joinpath(f'{DA.film}_{DA.sample_version}_qr{q_min}to{q_max}_{DA.incident_angle}.png'), \n",
    "    #                   dpi=150)\n",
    "    \n",
    "plt.show()\n",
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62b90b-919c-4dfe-91fe-4408eaf40ab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1D line fitting*\n",
    "Use lmfit to perform the linefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b5226-ef58-4d74-ba96-9d740206a620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import relevant lmfit functions:\n",
    "from lmfit.lineshapes import exponential, linear, pvoigt  # functions that return arrays based on defined function\n",
    "from lmfit import Model, Parameters, minimize  # lmfit pieces to combose parameters and run minimization fit (can use equivalent Minimizer class instead of minimize)\n",
    "from lmfit.model import save_modelresult, load_modelresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7897b2-e53a-4847-815f-2173ac95166a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a total function for the curves to be included in the data:\n",
    "\n",
    "# Total function in this case will have: 1 linear background, 1 exponential decay background and 6 pseudo-voigt peaks:\n",
    "# Peaks: lamella, backbone, 0p9, 1p2, alkyl, and pipi\n",
    "def total_func(x, \n",
    "               slope_lin, intercept_lin,\n",
    "               amp_exp, decay_exp,\n",
    "               amp_lamella, cen_lamella, sigma_lamella, fraction_lamella,\n",
    "               amp_backbone, cen_backbone, sigma_backbone, fraction_backbone,\n",
    "               amp_0p9, cen_0p9, sigma_0p9, fraction_0p9,\n",
    "               amp_1p2, cen_1p2, sigma_1p2, fraction_1p2,\n",
    "               amp_alkyl, cen_alkyl, sigma_alkyl, fraction_alkyl,\n",
    "               amp_pipi, cen_pipi, sigma_pipi, fraction_pipi):\n",
    "    \n",
    "    return (linear(x, slope_lin, intercept_lin) +\n",
    "            exponential(x, amp_exp, decay_exp) + \n",
    "            pvoigt(x, amp_lamella, cen_lamella, sigma_lamella, fraction_lamella) +\n",
    "            pvoigt(x, amp_backbone, cen_backbone, sigma_backbone, fraction_backbone) +\n",
    "            pvoigt(x, amp_0p9, cen_0p9, sigma_0p9, fraction_0p9) +\n",
    "            pvoigt(x, amp_1p2, cen_1p2, sigma_1p2, fraction_1p2) +\n",
    "            pvoigt(x, amp_alkyl, cen_alkyl, sigma_alkyl, fraction_alkyl) +\n",
    "            pvoigt(x, amp_pipi, cen_pipi, sigma_pipi, fraction_pipi))\n",
    "\n",
    "\n",
    "# Initialize function as an lmfit model object to fit directly\n",
    "model = Model(total_func, independent_vars=['x'])\n",
    "\n",
    "# Functions to plot inidividual fit components from parameters later on:\n",
    "def linear_pars(pars, x):\n",
    "    slope_lin, intercept_lin = pars[f'slope_lin'].value, pars[f'intercept_lin'].value\n",
    "    return linear(x, slope_lin, intercept_lin)\n",
    "\n",
    "def exponential_pars(pars, x):\n",
    "    amp_exp, decay_exp = pars[f'amp_exp'].value, pars[f'decay_exp'].value\n",
    "    return exponential(x, amp_exp, decay_exp)\n",
    "\n",
    "def pvoigt_peak(pars, x, peakname):\n",
    "    amp, cen, sigma, fraction = pars[f'amp_{peakname}'].value, pars[f'cen_{peakname}'].value, pars[f'sigma_{peakname}'].value, pars[f'fraction_{peakname}'].value\n",
    "    return pvoigt(x, amp, cen, sigma, fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbcdf3-dd0a-45fd-bd11-30d15cc15a14",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select data linecut, create parameters, run fit:\n",
    "# This cell is working for individual linecuts\n",
    "# Parameters used here are for linefits v2\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "### Select data\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "# chi_min, chi_width, chi_bins = [10, 6, 12]  \n",
    "chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "q_min = 0.25  # for full qr cut\n",
    "q_max = 1.78\n",
    "\n",
    "# Select attribute\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38'],\n",
    "#                        'incident_angle':['th0.080', 'th0.110']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26'],\n",
    "                       'incident_angle':['th0.120', 'th0.150']}\n",
    "\n",
    "# selected_attrs_dict = {'sample_ID': ['AL38']}\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL25', 'AL26'], 'incident_angle': ['th0.110']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "all_outs = {}\n",
    "good_ratios = {}\n",
    "bad_ratios = {}\n",
    "\n",
    "# Set constant background intercept value location parameters:\n",
    "bkg_x = 0.56\n",
    "bkg_extent = 0.015  # how far (plus and minus) around selected point to average \n",
    "for bkg_frac in tqdm([0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], desc='background values'):\n",
    "# for bkg_frac in tqdm([0.6, 0.7, 0.8, 0.9], desc='background values'):\n",
    "    for DA in tqdm(selected_DAs, desc='samples'):\n",
    "        sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "        # binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi')\n",
    "        binned_DA = sliced_DA.groupby_bins('chi', chi_bins).integrate('chi')\n",
    "\n",
    "        out = None  # clear any default parameters from previous sample\n",
    "\n",
    "        # Set path to save plots and lmfit results\n",
    "        fitsPath = outPath.joinpath('full_linefits_v2')\n",
    "        bkgFracPath = fitsPath.joinpath(f'background-q-{bkg_x}_q-extent-{bkg_extent}_intercept-val-frac-{bkg_frac}')\n",
    "        bkgFracPath.mkdir(exist_ok=True)\n",
    "        samplePath = bkgFracPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}')\n",
    "        samplePath.mkdir(exist_ok=True)\n",
    "\n",
    "        # for i, chi_bin in tqdm(enumerate(binned_DA.chi_bins.data), total=len(binned_DA.chi_bins.data), desc='chi_bins'):\n",
    "        for i, chi_bin in enumerate(binned_DA.chi_bins.data):\n",
    "            chi_key = f'{round(chi_bin.left)}-{round(chi_bin.right)}'\n",
    "\n",
    "            sel_DA = binned_DA.sel(chi_bins=chi_bin)  \n",
    "\n",
    "            x = sel_DA.qr.data\n",
    "            data = sel_DA.data\n",
    "\n",
    "\n",
    "            bkg_val = float(sel_DA.sel(qr=slice(bkg_x-0.02, bkg_x+0.02)).mean('qr')) * bkg_frac\n",
    "            # print(f'bkg value: {bkg_val}')\n",
    "\n",
    "            #  Initialize parameter values & constraints:\n",
    "            if out and out.redchi<100:  \n",
    "                # Use previous parameters if they're reasonable (low ish reduced chi value)\n",
    "                params = out.params\n",
    "                params.set(intercept_lin = {'value':bkg_val, 'min':bkg_val*0.85, 'max':bkg_val*1.15})\n",
    "                if chi_bin.left<10:\n",
    "                    params.set(amp_exp = {'value':0, 'vary':False})\n",
    "                if chi_bin.left<50:\n",
    "                    params.set(amp_backbone = {'value':0, 'vary':False})\n",
    "            else:   \n",
    "                # Otherwise, initialize new parameters \n",
    "                params = Parameters()   \n",
    "                params.add(f'slope_lin',         value=0,     vary=False)\n",
    "                params.add(f'intercept_lin',     value=bkg_val, min=bkg_val*0.9, max=bkg_val*1.1)\n",
    "                if chi_bin.left<10:\n",
    "                    params.add(f'amp_exp',           value=10,      min=0.01,  max=1e2, vary=False)\n",
    "                    params.add(f'decay_exp',         value=0.1,     min=0.01,  max=1e2, vary=False)\n",
    "                else:\n",
    "                    params.add(f'amp_exp',           value=0,     vary=False)\n",
    "                    params.add(f'decay_exp',         value=0,     vary=False)\n",
    "                params.add(f'amp_lamella',       value=250,   min=0,     max=1e4)\n",
    "                params.add(f'cen_lamella',       value=0.31,  min=0.28,  max=0.34)\n",
    "                params.add(f'sigma_lamella',     value=0.05,  min=0.01,  max=0.2)\n",
    "                params.add(f'fraction_lamella',  value=0.5,   min=0,     max=1)\n",
    "                if chi_bin.left<50:\n",
    "                    params.add(f'amp_backbone',      value=0, vary=False)\n",
    "                else:\n",
    "                    params.add(f'amp_backbone',      value=0.005, min=0,     max=10)\n",
    "                params.add(f'cen_backbone',      value=0.66,  min=0.63,  max=0.7)\n",
    "                params.add(f'sigma_backbone',    value=1e-3,  min=0.01,  max=0.05)\n",
    "                params.add(f'fraction_backbone', value=0.5,   min=0,     max=1)\n",
    "                params.add(f'amp_0p9',           value=1,     min=0,     max=10)\n",
    "                params.add(f'cen_0p9',           value=0.93,  min=0.85,  max=0.99)\n",
    "                params.add(f'sigma_0p9',         value=0.02,  min=0.01,  max=0.4)\n",
    "                params.add(f'fraction_0p9',      value=0.5,   min=0,     max=1)\n",
    "                params.add(f'amp_1p2',           value=1,     min=0,     max=10)\n",
    "                params.add(f'cen_1p2',           value=1.2,   min=1.15,  max=1.25)\n",
    "                params.add(f'sigma_1p2',         value=0.5,   min=0.05,  max=0.5)\n",
    "                params.add(f'fraction_1p2',      value=0.5,   min=0,     max=1)\n",
    "                params.add(f'amp_alkyl',         value=50,    min=0,     max=400)\n",
    "                params.add(f'cen_alkyl',         value=1.4,   min=1.35,  max=1.45)\n",
    "                params.add(f'sigma_alkyl',       value=0.5,   min=0.1,   max=0.5)\n",
    "                params.add(f'fraction_alkyl',    value=0.5,   min=0,     max=1)\n",
    "                params.add(f'amp_pipi',          value=100,   min=0,     max=1000)\n",
    "                params.add(f'cen_pipi',          value=1.73,  min=1.65,  max=1.8)\n",
    "                params.add(f'sigma_pipi',        value=0.1,   min=0.05,  max=0.3)\n",
    "                params.add(f'fraction_pipi',     value=0.5,   min=0,     max=1)    \n",
    "\n",
    "            ### Run minimization / fit:\n",
    "            out = model.fit(data, params, x=x)\n",
    "\n",
    "            # Retry up to some limit number of times for better fit result\n",
    "            limit = 10\n",
    "            counter = 0\n",
    "            while out.redchi>50:\n",
    "                # bad fit, retry\n",
    "                out = model.fit(data, out.params, x=x)\n",
    "                counter += 1\n",
    "                if counter >= limit:\n",
    "                    break            \n",
    "\n",
    "            # print(f'Retried fit {counter} times...')\n",
    "\n",
    "\n",
    "            # # Save lmfit model result:\n",
    "            # save_modelresult(out, samplePath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_{chi_key}.json'))\n",
    "\n",
    "            # Plot:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            lin_bkg = linear_pars(out.params, x)\n",
    "            exp_bkg = exponential_pars(out.params, x)\n",
    "            pvoigt1 = pvoigt_peak(out.params, x, 'lamella')\n",
    "            pvoigt2 = pvoigt_peak(out.params, x, 'backbone')\n",
    "            pvoigt3 = pvoigt_peak(out.params, x, '0p9')\n",
    "            pvoigt4 = pvoigt_peak(out.params, x, '1p2')\n",
    "            pvoigt5 = pvoigt_peak(out.params, x, 'alkyl')\n",
    "            pvoigt6 = pvoigt_peak(out.params, x, 'pipi')\n",
    "\n",
    "            ax.plot(x, data, 'o')\n",
    "            ax.plot(x, out.best_fit, '-', label='full_fit')\n",
    "            ax.plot(x, lin_bkg, label=f'lin_bkg')\n",
    "            ax.plot(x, exp_bkg, label=f'exp_bkg')\n",
    "            ax.plot(x, pvoigt1, label=f'lamella')\n",
    "            ax.plot(x, pvoigt2, label=f'backbone')\n",
    "            ax.plot(x, pvoigt3, label=f'0p9')\n",
    "            ax.plot(x, pvoigt4, label=f'1p2')\n",
    "            ax.plot(x, pvoigt5, label=f'alkyl')\n",
    "            ax.plot(x, pvoigt6, label=f'pipi')\n",
    "            ax.set_title(f'{sn[sel_DA.sample_ID]} chi bin: {chi_bin}')\n",
    "            ax.legend(title=f'redchi={np.round(out.redchi, 1)}, abs_resid_mean={np.round(np.abs(out.residual).mean(), 1)}, resid_mean={np.round(out.residual.mean(), 1)}')\n",
    "\n",
    "            # fig.savefig(samplePath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_{chi_key}.png'))\n",
    "\n",
    "            plt.show()\n",
    "            plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe388054-0350-455a-8333-51554b941e85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009dee8-2a02-4a7d-826b-d2b8ea268bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot pseudovoigt fractions, d-spacings, scherrer coherence lengths, and normalized peak areas vs chi\n",
    "# For each background default value\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "fitsPath = outPath.joinpath('full_linefits_v2')\n",
    "bkg_all_params = {}\n",
    "bkg_extracted_value_arrays = {}\n",
    "for bkgFracPath in tqdm(sorted(fitsPath.glob('background*extent-0.015*'))):\n",
    "# for bkgFracPath in tqdm(sorted(fitsPath.glob('background*'))):\n",
    "    # Extract background fraction value from folder name \n",
    "    bkg_frac = bkgFracPath.name.split('_')[-1].split('-')[-1]\n",
    "    # print(bkg_frac)\n",
    "    \n",
    "    # Get parameters from saved fit results\n",
    "    all_params = {}\n",
    "    for measurementPath in sorted(bkgFracPath.glob('PM6*')):\n",
    "        measurement_name = measurementPath.name  # get fit name\n",
    "        \n",
    "        # Load lmfit results\n",
    "        params = {}\n",
    "        for saved_result in sorted(measurementPath.glob('*.json')):\n",
    "            result_name = saved_result.stem  # stem doesn't include filetype format at end\n",
    "            chi_bin_key = result_name.split('_')[-1]\n",
    "\n",
    "            result = load_modelresult(saved_result)\n",
    "\n",
    "            #only load for chi bins with good fit parameters (low reduced chi and residual average close to zero)\n",
    "            # if result.redchi<1000 and np.abs(result.residual.mean())<1:\n",
    "            # if result.redchi<3000:\n",
    "            params[chi_bin_key] = result.params.valuesdict()\n",
    "\n",
    "        all_params[measurement_name] = params\n",
    "        \n",
    "    bkg_all_params[bkg_frac] = all_params\n",
    "\n",
    "    # # Choose sample sets (by folder/sample name) (this should probably be outside this loop):\n",
    "    # peak = 'lamella'\n",
    "    \n",
    "    \n",
    "    # incident_angle = 'th0.110'\n",
    "    # PM6_CF_samples = [f'PM6_0CN-CF_x0.000_{incident_angle}', f'PM6_p5CN-CF_x0.000_{incident_angle}', f'PM6_1CN-CF_x2.000_{incident_angle}', f'PM6_5CN-CF_x2.000_{incident_angle}']\n",
    "    # PM6_CB_samples = [f'PM6_0CN-CB_x0.000_{incident_angle}', f'PM6_p5CN-CB_x2.000_{incident_angle}', f'PM6_1CN-CB_x0.000_{incident_angle}', f'PM6_5CN-CB_x0.000_{incident_angle}']\n",
    "\n",
    "\n",
    "    # Define custom colors for each film, here I've chosen just two shades along the same sequention colorbar for each sample group\n",
    "    # colors = plt.cm.viridis_r(np.linspace(0,1,len(PM6_CF_samples)))\n",
    "    # colors = plt.cm.viridis_r(np.linspace(0,1,4))\n",
    "    # colors = plt.cm.viridis_r(np.linspace(0,1,3))\n",
    "\n",
    "    # ### Extracting info from all_params & populating ___ vs chi plots:\n",
    "    # summed_peak_areas = {}\n",
    "    # avg_dspacings = {}\n",
    "    # avg_peak_centers = {}\n",
    "    # avg_coherence_lengths = {}\n",
    "    \n",
    "    extracted_value_arrays = {} \n",
    "    for i, film in enumerate(all_params.keys()):\n",
    "    # for i, film in enumerate(PM6_CF_samples):\n",
    "    # for i, film in enumerate(PM6_CB_samples):\n",
    "        extracted_arrays = {}\n",
    "        for peak in ['lamella', 'pipi']:\n",
    "            peak_areas = []\n",
    "            pvoigt_fracs = []\n",
    "            peak_centers = []\n",
    "            peak_fwhms = []\n",
    "\n",
    "            chis = []\n",
    "            for chi_bin, params in all_params[film].items():   \n",
    "                # Get starting value of chi bin\n",
    "                chi = int(chi_bin.split('-')[0]) + 2  # set to midpoint of values\n",
    "                chis.append(chi)\n",
    "                # Get peak areas\n",
    "                peak_area = params[f'amp_{peak}']\n",
    "                peak_areas.append(peak_area)\n",
    "                # Get pseudovoigt fraction\n",
    "                pvoigt_frac = params[f'fraction_{peak}']\n",
    "                pvoigt_fracs.append(pvoigt_frac)\n",
    "                # Get peak centers\n",
    "                peak_center = params[f'cen_{peak}']\n",
    "                peak_centers.append(peak_center)\n",
    "                # Get peak fwhms\n",
    "                peak_fwhm = params[f'sigma_{peak}'] * 2\n",
    "                peak_fwhms.append(peak_fwhm)\n",
    "\n",
    "            # Get normalized peak areas (divide by sum of all peak areas; adds to 1) and write area sum to dict\n",
    "            peak_areas = np.array(peak_areas)\n",
    "            peak_sum = np.sum(peak_areas)\n",
    "            # summed_peak_areas[film] = np.sum(peak_areas)  # Write sum of peak area out, this corresponds to relative extent of crystallinity if thickness normalized\n",
    "            normed_peak_areas = peak_areas / np.sum(peak_areas)\n",
    "            \n",
    "            extracted_arrays[f'{peak}_peak_sum'] = peak_sum\n",
    "            extracted_arrays[f'{peak}_normed_peak_areas'] = normed_peak_areas \n",
    "\n",
    "            # Calculate d-spacings from peak centers, calculate peak-area-weighted average of d-spacing\n",
    "            dspacings = (2*np.pi) / np.array(peak_centers) \n",
    "            # avg_peak_centers[film] = peak_center\n",
    "            avg_dspacing = np.round(np.sum(dspacings*normed_peak_areas), 2)\n",
    "            # avg_dspacings[film] = avg_dspacing  \n",
    "\n",
    "            extracted_arrays[f'{peak}_dspacings'] = dspacings \n",
    "\n",
    "\n",
    "            # Calculate coherence length from peak fwhm, calculate peak-area-weighted average of coherence length    \n",
    "            coherence_lengths = (2*np.pi*0.9) / np.array(peak_fwhms) \n",
    "            avg_coherence_length = np.round(np.sum(coherence_lengths*normed_peak_areas), 2)\n",
    "            # avg_coherence_lengths[film] = avg_coherence_length\n",
    "\n",
    "            extracted_arrays[f'{peak}_coherence_lengths'] = coherence_lengths \n",
    "\n",
    "\n",
    "        extracted_value_arrays[film] = extracted_arrays\n",
    "\n",
    "    bkg_extracted_value_arrays[bkg_frac] = extracted_value_arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e9ba0-3e9c-4a70-8263-421f1dbcd810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bkg_extracted_value_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebb859-2d90-4d89-99e0-3005a2adfc38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def average_and_error_bars_with_padding(nested_dict):\n",
    "#     # Initialize dictionaries to hold results\n",
    "#     average_dict = {}\n",
    "#     stddev_dict = {}\n",
    "\n",
    "#     # Loop over measurement names\n",
    "#     for measurement_name in next(iter(nested_dict.values())):\n",
    "#         average_dict[measurement_name] = {}\n",
    "#         stddev_dict[measurement_name] = {}\n",
    "        \n",
    "#         # Define array dictionary with keys for each parameter\n",
    "#         arrays_dict = {\n",
    "#             'lamella_normed_peak_areas': [], 'lamella_dspacings': [], 'lamella_coherence_lengths': [],\n",
    "#             'pipi_normed_peak_areas': [], 'pipi_dspacings': [], 'pipi_coherence_lengths': []\n",
    "#         }\n",
    "\n",
    "#         # Gather arrays for each parameter across background fractions\n",
    "#         for background_fraction in nested_dict.keys():\n",
    "#             for param in arrays_dict:\n",
    "#                 # Append arrays for this background fraction\n",
    "#                 arrays_dict[param].append(nested_dict[background_fraction][measurement_name][param])\n",
    "\n",
    "#         # Process each parameter\n",
    "#         for param, array_list in arrays_dict.items():\n",
    "#             # Get the maximum length across all arrays for this parameter\n",
    "#             max_length = max(arr.shape[0] for arr in array_list)\n",
    "\n",
    "#             # Pad each array in array_list to the maximum length with NaN\n",
    "#             padded_arrays = [np.pad(arr, (0, max_length - arr.shape[0]), constant_values=np.nan) for arr in array_list]\n",
    "            \n",
    "#             # Stack padded arrays along a new dimension and calculate mean/std deviation while ignoring NaN\n",
    "#             stacked_array = np.stack(padded_arrays, axis=0)\n",
    "#             average_dict[measurement_name][param] = np.nanmean(stacked_array, axis=0)\n",
    "#             stddev_dict[measurement_name][param] = np.nanstd(stacked_array, axis=0)\n",
    "            \n",
    "#         # Now also process the peak sum for rDoC :)\n",
    "        \n",
    "\n",
    "#     return average_dict, stddev_dict\n",
    "\n",
    "# # Example usage\n",
    "# avg_values, err_values = average_and_error_bars_with_padding(bkg_extracted_value_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ea16c-598a-412e-b5bb-5db8192371b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1232b-0b7d-403b-8a7d-7baab05c8637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[arr.shape for arr in bkg_extracted_value_arrays['0.3']['PM6_5CN-CF_x0.000_th0.110'].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e34aa-4eeb-4f6a-b2d0-0d80ea830946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[arr.shape for arr in avg_values['PM6_5CN-CF_x0.000_th0.110'].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58714ac-2782-423d-9462-5d49027de65a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def average_and_error_bars_for_arrays(nested_dict):\n",
    "    # Initialize dictionaries to hold results\n",
    "    average_dict = {}\n",
    "    stddev_dict = {}\n",
    "\n",
    "    # Loop over measurement names\n",
    "    for measurement_name in next(iter(nested_dict.values())):\n",
    "        average_dict[measurement_name] = {}\n",
    "        stddev_dict[measurement_name] = {}\n",
    "        \n",
    "        # Gather arrays for each parameter across background fractions\n",
    "        # arrays_dict = {'normed_peak_areas': [], 'dspacings': [], 'coherence_lengths': []}\n",
    "        arrays_dict = {'lamella_normed_peak_areas': [], 'lamella_dspacings': [], 'lamella_coherence_lengths': [],\n",
    "                       'pipi_normed_peak_areas': [], 'pipi_dspacings': [], 'pipi_coherence_lengths': []}\n",
    "\n",
    "        for background_fraction in nested_dict.keys():\n",
    "            for param in arrays_dict:\n",
    "                # Append arrays for this background fraction\n",
    "                arrays_dict[param].append(nested_dict[background_fraction][measurement_name][param])\n",
    "\n",
    "        # Stack arrays along the new axis to calculate mean and std deviation\n",
    "        for param, array_list in arrays_dict.items():\n",
    "            stacked_array = np.stack(array_list, axis=0)  # Stack along new dimension (background fractions)\n",
    "            average_dict[measurement_name][param] = np.mean(stacked_array, axis=0)  # Average along background fraction axis\n",
    "            stddev_dict[measurement_name][param] = np.std(stacked_array, axis=0)  # Stddev along background fraction axis\n",
    "            \n",
    "        # Now also do for peak sum (-> rDoC)\n",
    "        sum_arrays_dict = {'lamella_peak_sum': [], 'pipi_peak_sum': []}\n",
    "        for background_fraction in nested_dict.keys():\n",
    "            for param in sum_arrays_dict:\n",
    "                # Append arrays for this background fraction\n",
    "                sum_arrays_dict[param].append(nested_dict[background_fraction][measurement_name][param])\n",
    "\n",
    "        # Stack arrays along the new axis to calculate mean and std deviation\n",
    "        for param, sum_array_list in sum_arrays_dict.items():\n",
    "            stacked_array = np.stack(sum_array_list, axis=0)  # Stack along new dimension (background fractions)\n",
    "            average_dict[measurement_name][param] = np.mean(stacked_array, axis=0)  # Average along background fraction axis\n",
    "            stddev_dict[measurement_name][param] = np.std(stacked_array, axis=0)  # Stddev along background fraction axis        \n",
    "\n",
    "    return average_dict, stddev_dict\n",
    "\n",
    "avg_values, err_values = average_and_error_bars_for_arrays(bkg_extracted_value_arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0ad87-6627-4d27-947d-3effed453748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36505ba3-1e4e-4861-960b-4fb47a80e0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "err_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549b609-efa7-4933-aeda-018cb0caa1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def average_and_error_bars(nested_dict):\n",
    "#     # Initialize dictionaries to hold results\n",
    "#     average_dict = {}\n",
    "#     stddev_dict = {}\n",
    "\n",
    "#     # Loop over measurement names\n",
    "#     for measurement_name, chi_bins in next(iter(nested_dict.values())).items():\n",
    "#         average_dict[measurement_name] = {}\n",
    "#         stddev_dict[measurement_name] = {}\n",
    "\n",
    "#         # Loop over chi bins\n",
    "#         for chi_bin, _ in chi_bins.items():\n",
    "#             # Gather parameter values for each background fraction\n",
    "#             params_dict = {}\n",
    "#             for background_fraction in nested_dict:\n",
    "#                 fit_results = nested_dict[background_fraction][measurement_name][chi_bin]\n",
    "#                 for param, value in fit_results.items():\n",
    "#                     if param not in params_dict:\n",
    "#                         params_dict[param] = []\n",
    "#                     params_dict[param].append(value)\n",
    "\n",
    "#             # Compute average and standard deviation for each parameter\n",
    "#             average_dict[measurement_name][chi_bin] = {}\n",
    "#             stddev_dict[measurement_name][chi_bin] = {}\n",
    "#             for param, values in params_dict.items():\n",
    "#                 average_dict[measurement_name][chi_bin][param] = np.mean(values)\n",
    "#                 stddev_dict[measurement_name][chi_bin][param] = np.std(values)\n",
    "\n",
    "#     return average_dict, stddev_dict\n",
    "\n",
    "# # Example usage\n",
    "# avg_params, err_params = average_and_error_bars(bkg_all_params)\n",
    "\n",
    "# # Output the results\n",
    "# print(\"Averages:\\n\", avg)\n",
    "# print(\"\\nStandard Deviations (Error Bars):\\n\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f99a15-a423-45df-a729-287e180aa488",
   "metadata": {},
   "source": [
    "### Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b7bb8-9a6f-4fe1-aa34-c01410591a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.cm.Greys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b0638-d377-418d-bc47-a8e630b1be1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "# peak = 'pipi'\n",
    "for peak in ['lamella', 'pipi']:\n",
    "# for peak in ['pipi']:\n",
    "    for solvent in ['CF', 'CB']:\n",
    "    # for solvent in ['CB']:  # dont yet have .120 and .150 CB\n",
    "        avgs_dict = {}\n",
    "        errs_dict = {}\n",
    "        for incident_angle in ['th0.080', 'th0.110', 'th0.120', 'th0.150']:\n",
    "            # # Initialize figures to be populated later\n",
    "            # fig_dspacing, ax_dspacing = plt.subplots(figsize=((4.5,2.5)), dpi=150, constrained_layout=True)\n",
    "            # fig_ccl, ax_ccl = plt.subplots(figsize=((4.5,2.5)), dpi=150, constrained_layout=True)\n",
    "            # fig_pole, ax_pole = plt.subplots(figsize=((4.5,2.5)), dpi=150, constrained_layout=True)\n",
    "            \n",
    "\n",
    "            alphai = float(incident_angle[2:])\n",
    "            alpha = '$α_{inc}$'\n",
    "\n",
    "            if solvent == 'CF':\n",
    "                PM6_samples = [f'PM6_0CN-CF_x0.000_{incident_angle}', f'PM6_p5CN-CF_x0.000_{incident_angle}', f'PM6_1CN-CF_x2.000_{incident_angle}', f'PM6_5CN-CF_x2.000_{incident_angle}']\n",
    "            elif solvent == 'CB':\n",
    "                PM6_samples = [f'PM6_0CN-CB_x0.000_{incident_angle}', f'PM6_p5CN-CB_x2.000_{incident_angle}', f'PM6_1CN-CB_x0.000_{incident_angle}', f'PM6_5CN-CB_x0.000_{incident_angle}']\n",
    "\n",
    "            additive_label = ['0.0% CN', '0.5% CN', '1.0% CN', '5.0% CN']\n",
    "\n",
    "            # Define custom colors for each film, here I've chosen just two shades along the same sequention colorbar for each sample group\n",
    "            # colors = plt.cm.viridis_r(np.linspace(0,1,len(PM6_CF_samples)))\n",
    "            colors = plt.cm.viridis_r(np.linspace(0,1,4))\n",
    "            # Define custom colors for each parameter to plot in line plot summary\n",
    "            param_colors = plt.cm.plasma(np.linspace(0, 0.9, 3))  # orientation, ccl, d-spacing\n",
    "\n",
    "            ### Extracting info from all_params & populating ___ vs chi plots:\n",
    "            peak_sums = []\n",
    "            peak_sum_errs = []\n",
    "            avg_chis = []\n",
    "            avg_Ss = []\n",
    "            avg_ccls = []\n",
    "            avg_dspacings = []\n",
    "            avg_chi_errs = []\n",
    "            avg_ccl_errs = []\n",
    "            avg_dspacing_errs = []\n",
    "            # for i, film in enumerate(all_params.keys()):\n",
    "\n",
    "            chis = np.array([12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80])\n",
    "\n",
    "            for i, film in enumerate(PM6_samples):\n",
    "\n",
    "                # Get normalized peak areas from averaged dictionary\n",
    "                normed_peak_areas = avg_values[film][f'{peak}_normed_peak_areas']\n",
    "                normed_peak_errs = err_values[film][f'{peak}_normed_peak_areas']\n",
    "                avg_chi = np.round((normed_peak_areas*chis).sum(), 2)\n",
    "                avg_fz = np.round((normed_peak_areas*np.cos(np.deg2rad(chis))**2).sum(), 2)\n",
    "                # avg_fz = np.round((normed_peak_areas*np.deg2rad(chis)*np.cos(np.deg2rad(chis))**2).sum(), 2)\n",
    "                avg_S = (1/2) * ((3 * avg_fz) - 1)\n",
    "                avg_Ss.append(avg_S)\n",
    "                # avg_chi_err = np.round((normed_peak_errs*chis).mean(), 2)  # this should actually be mean() instead of sum?\n",
    "                avg_chi_err = np.round(np.sqrt(np.sum((chis-avg_chi)**2 * normed_peak_errs**2)), 2)  # error propogation from GPT\n",
    "                avg_chis.append(avg_chi)\n",
    "                avg_chi_errs.append(avg_chi_err)\n",
    "                \n",
    "                # Get peak sum:\n",
    "                peak_sum = avg_values[film][f'{peak}_peak_sum']\n",
    "                peak_sum_err = err_values[film][f'{peak}_peak_sum']\n",
    "                peak_sums.append(peak_sum)\n",
    "                peak_sum_errs.append(peak_sum_err)\n",
    "            \n",
    "                # Get coherence lengths from averaged dictionary\n",
    "                coherence_lengths = avg_values[film][f'{peak}_coherence_lengths']\n",
    "                coherence_length_errs = err_values[film][f'{peak}_coherence_lengths']\n",
    "                avg_ccl = np.round((coherence_lengths*normed_peak_areas).sum(), 2)  \n",
    "                avg_ccl_err = np.round((coherence_length_errs*normed_peak_areas).sum(), 4)\n",
    "                # avg_ccl = np.round(coherence_lengths.mean(), 2)\n",
    "                # avg_ccl_err = np.round(coherence_length_errs.mean(), 2)\n",
    "                avg_ccls.append(avg_ccl)\n",
    "                avg_ccl_errs.append(avg_ccl_err)\n",
    "                \n",
    "                # Get dspacings from averaged dictionary\n",
    "                dspacings = avg_values[film][f'{peak}_dspacings']\n",
    "                dspacing_errs = err_values[film][f'{peak}_dspacings']\n",
    "                avg_dspacing = np.round((dspacings*normed_peak_areas).sum(), 2)\n",
    "                avg_dspacing_err = np.round((dspacing_errs*normed_peak_areas).sum(), 2)\n",
    "                # avg_dspacing = np.round(dspacings.mean(), 2)\n",
    "                # avg_dspacing_err = np.round(dspacing_errs.mean(), 2)\n",
    "                avg_dspacings.append(avg_dspacing)\n",
    "                avg_dspacing_errs.append(avg_dspacing_err)\n",
    "                                \n",
    "                ## Plotting\n",
    "                # Plot d-spacing vs chi for each film\n",
    "                ax_dspacing.errorbar(chis, dspacings, label=f'{additive_label[i]}: {avg_dspacing} ± {avg_dspacing_err}', color=colors[i], marker='.', \n",
    "                                     capsize=4, yerr=dspacing_errs)\n",
    "                ax_dspacing.set(title=f'PM6 {solvent}, {alpha}={alphai}°: {peak} d-spacing vs $\\\\chi$',\n",
    "                                ylabel='d-spacing [Å]',\n",
    "                                xlabel='$\\\\chi$ value [°]')\n",
    "                                # ylim=(3.55,3.8))\n",
    "                ax_dspacing.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.9))\n",
    "                ax_dspacing.grid(visible=True,which='major',axis='y')\n",
    "\n",
    "\n",
    "\n",
    "                # Plot coherence length vs chi for each film\n",
    "                if peak == 'pipi' and additive_label[i]=='5.0% CN' and False:\n",
    "                    ax_ccl.errorbar(chis[:10], coherence_lengths[:10], label=f'{additive_label[i]}: {avg_ccl} ± {avg_ccl_err}', color=colors[i], marker='.', \n",
    "                                    capsize=4, yerr=coherence_length_errs[:10])\n",
    "                    ax_ccl.set(title=f'PM6 {solvent}, {alpha}={alphai}°: {peak} CCL vs $\\\\chi$',\n",
    "                               ylabel='CCL [Å]',\n",
    "                               xlabel='$\\\\chi$ value [°]')\n",
    "                               # ylim=(13,18.5))\n",
    "                    ax_ccl.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.9))\n",
    "                    ax_ccl.grid(visible=True,which='major',axis='y')\n",
    "                else:\n",
    "                    ax_ccl.errorbar(chis, coherence_lengths, label=f'{additive_label[i]}: {avg_ccl} ± {avg_ccl_err}', color=colors[i], marker='.', \n",
    "                                    capsize=4, yerr=coherence_length_errs)\n",
    "                    ax_ccl.set(title=f'PM6 {solvent}, {alpha}={alphai}°: {peak} CCL vs $\\\\chi$',\n",
    "                               ylabel='CCL [Å]',\n",
    "                               xlabel='$\\\\chi$ value [°]')\n",
    "                               # ylim=(13,18.5))\n",
    "                    ax_ccl.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.9))\n",
    "                    ax_ccl.grid(visible=True,which='major',axis='y')\n",
    "                    \n",
    "                if peak == 'pipi':\n",
    "                    ax_ccl.set_ylim((5,35))\n",
    "                else:\n",
    "                    ax_ccl.set_ylim((25,175))\n",
    "\n",
    "                # Plot 'pole figure' for each film\n",
    "                ax_pole.errorbar(chis, normed_peak_areas, label=f'{additive_label[i]}: {avg_chi} ± {avg_chi_err}', color=colors[i], marker='.',\n",
    "                                 capsize=4, yerr=normed_peak_errs)\n",
    "                ax_pole.set(title=f'PM6 {solvent}, {alpha}={alphai}°: {peak} pole figure',\n",
    "                            ylabel='Peak area [arb. units]',\n",
    "                            xlabel='$\\\\chi$ value [°]')\n",
    "                ax_pole.set_ybound(lower=-0.005, upper=0.17)\n",
    "                ax_pole.grid(visible=True,which='major',axis='y')\n",
    "                # ax_pole.legend(title='Film')    \n",
    "                ax_pole.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.9))\n",
    "\n",
    "                # # Save\n",
    "                # savePath = outPath.joinpath('giwaxs_plots/linefit_summaries_v3')\n",
    "                # fig_dspacing.savefig(savePath.joinpath(f'PM6_{solvent}_{peak}_dspacings_{incident_angle}.png'), dpi=120, bbox_inches='tight')\n",
    "                # fig_ccl.savefig(savePath.joinpath(f'PM6_{solvent}_{peak}_coherence_lengths_{incident_angle}.png'), dpi=120, bbox_inches='tight')\n",
    "                # fig_pole.savefig(savePath.joinpath(f'PM6_{solvent}_{peak}_peak_areas_{incident_angle}.png'), dpi=120, bbox_inches='tight')\n",
    "\n",
    "            plt.show()\n",
    "                \n",
    "            avgs_dict[f'{incident_angle}_chis'] = avg_chis\n",
    "            # avgs_dict[f'{incident_angle}_Ss'] = avg_Ss\n",
    "            avgs_dict[f'{incident_angle}_ccls'] = avg_ccls\n",
    "            avgs_dict[f'{incident_angle}_dspacings'] = avg_dspacings\n",
    "            avgs_dict[f'{incident_angle}_sums'] = peak_sums\n",
    "            errs_dict[f'{incident_angle}_chi_errs'] = avg_chi_errs\n",
    "            errs_dict[f'{incident_angle}_ccl_errs'] = avg_ccl_errs\n",
    "            errs_dict[f'{incident_angle}_dspacing_errs'] = avg_dspacing_errs\n",
    "            errs_dict[f'{incident_angle}_sum_errs'] = peak_sum_errs\n",
    "            \n",
    "            fig_summary, ax1_summary = plt.subplots(figsize=(5.5,2.5), dpi=150, tight_layout=False)\n",
    "            ax1_summary.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "                                 avg_dspacings, \n",
    "                                 yerr=avg_dspacing_errs, \n",
    "                                 color=param_colors[2],\n",
    "                                 marker='o',\n",
    "                                 capsize=4,\n",
    "                                 label='d-spacing') \n",
    "            ax1_summary.set_ylabel('d-spacing [Å]')\n",
    "            if peak == 'lamella':\n",
    "                ax1_summary.set_ylim((18.5,21.5))\n",
    "            else:\n",
    "            # if peak == 'pipi':\n",
    "                ax1_summary.set_ylim((3.5,3.7))\n",
    "\n",
    "            ax2_summary = ax1_summary.twinx()\n",
    "            ax2_summary.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "                                 avg_ccls, \n",
    "                                 yerr=avg_ccl_errs, \n",
    "                                 color=param_colors[1],\n",
    "                                 marker='o',\n",
    "                                 capsize=4,\n",
    "                                 label='CCL')\n",
    "            ax2_summary.set_ylabel('CCL [Å]')\n",
    "            if peak == 'lamella':\n",
    "                ax2_summary.set_ylim((20,130))\n",
    "            else:\n",
    "            # if peak == 'pipi':\n",
    "                ax2_summary.set_ylim((5,30))\n",
    "                \n",
    "            \n",
    "                \n",
    "            ax3_summary = ax1_summary.twinx()\n",
    "            ax3_summary.spines['right'].set_position(('outward', 45))  # Offset the third axis\n",
    "            ax3_summary.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "                                 avg_chis, \n",
    "                                 yerr=avg_chi_errs, \n",
    "                                 color=param_colors[0],\n",
    "                                 marker='o',\n",
    "                                 capsize=4,\n",
    "                                 label='Stacking angle')\n",
    "            ax3_summary.set_ylabel('Stacking angle χ [°]')\n",
    "            ax3_summary.set_xlabel('CN additive [vol %]')\n",
    "            ax3_summary.set_ylim((20,70))\n",
    "\n",
    "            ax4_summary = ax1_summary.twinx()\n",
    "            ax4_summary.spines['right'].set_position(('outward', 90))  # Offset the third axis\n",
    "            ax4_summary.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "                                 avg_Ss, \n",
    "                                 # yerr=avg_chi_errs, \n",
    "                                 color='orange',\n",
    "                                 marker='o',\n",
    "                                 capsize=4,\n",
    "                                 label='S')\n",
    "            ax4_summary.set_ylabel('Uniaxial order parameter S')\n",
    "            ax4_summary.set_xlabel('CN additive [vol %]')\n",
    "            ax4_summary.set_ylim((-0.5,1))\n",
    "\n",
    "            fig_summary.legend(loc='upper right', bbox_to_anchor=[1.45,0.62])\n",
    "            fig_summary.suptitle(f'PM6 {solvent}, {alpha}={alphai}°\\n Average {peak} fit parameters', y=0.9)\n",
    "\n",
    "            # # Save\n",
    "            # savePath = outPath.joinpath('giwaxs_plots/linefit_summaries_v3')\n",
    "            # fig_summary.savefig(savePath.joinpath(f'PM6_{solvent}_{peak}_{incident_angle}_summary.png'), dpi=150, bbox_inches='tight')\n",
    "\n",
    "            \n",
    "            plt.show()\n",
    "            plt.close('all')  \n",
    "\n",
    "#         # display(avgs_dict)\n",
    "#         # display(errs_dict)\n",
    "        \n",
    "        aoi_colors = plt.cm.winter_r(np.linspace(0,1,4))\n",
    "        # aoi_colors = plt.cm.turbo(np.linspace(0,1,4))\n",
    "        colors_dict = {\n",
    "            'th0.080': aoi_colors[0],\n",
    "            'th0.110': aoi_colors[1],\n",
    "            'th0.120': aoi_colors[2],\n",
    "            'th0.150': aoi_colors[3],\n",
    "        }\n",
    "        sums_colors = plt.cm.Greys(np.linspace(0.35,1,4))\n",
    "        sums_colors_dict = {\n",
    "            'th0.080': sums_colors[0],\n",
    "            'th0.110': sums_colors[1],\n",
    "            'th0.120': sums_colors[2],\n",
    "            'th0.150': sums_colors[3],\n",
    "        }\n",
    "        linestyles_dict = {\n",
    "            'ccls': ':',\n",
    "            'chis': '-',\n",
    "            'sums': '--'\n",
    "        }\n",
    "            \n",
    "                \n",
    "        fig_aois, ax1_aois = plt.subplots(figsize=(2,2), dpi=150)\n",
    "        ax2_aois = None\n",
    "        ax3_aois = None\n",
    "        for angle_ptype, avgs, errs in zip(avgs_dict.keys(), avgs_dict.values(), errs_dict.values()):\n",
    "            aoi = angle_ptype.split('_')[0]\n",
    "            ptype = angle_ptype.split('_')[1]\n",
    "            \n",
    "            alphai = float(aoi[2:])\n",
    "            alpha = '$α_{inc}$'\n",
    "                        \n",
    "            if ptype == 'chis':\n",
    "                ax1_aois.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "                                     avgs, \n",
    "                                     yerr=errs, \n",
    "                                     color=colors_dict[aoi],\n",
    "                                     marker='.',\n",
    "                                     linestyle=linestyles_dict[ptype],\n",
    "                                     capsize=2,\n",
    "                                     label=f'Stacking angle: {alpha}={alphai}°')\n",
    "                ax1_aois.set_ylabel('Stacking angle χ [°]')\n",
    "                ax1_aois.set_xlabel('CN additive [vol %]')\n",
    "\n",
    "            if peak == 'lamella' and ptype == 'ccls':\n",
    "            # if ptype == 'ccls':            \n",
    "                if ax2_aois is None:\n",
    "                    ax2_aois = ax1_aois.twinx()\n",
    "\n",
    "                ax2_aois.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "                                     avgs, \n",
    "                                     yerr=errs, \n",
    "                                     color=colors_dict[aoi],\n",
    "                                     linestyle=linestyles_dict[ptype],\n",
    "                                     marker='.',\n",
    "                                     capsize=2,\n",
    "                                     label=f'CCL: {alpha}={alphai}°')\n",
    "                ax2_aois.set_ylabel('CCL [Å]')\n",
    "            elif peak == 'pipi' and ptype == 'ccls':\n",
    "                if ax2_aois is None:\n",
    "                    ax2_aois = ax1_aois.twinx()\n",
    "\n",
    "                ax2_aois.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "                                     avgs, \n",
    "                                     yerr=errs, \n",
    "                                     color=colors_dict[aoi],\n",
    "                                     linestyle=linestyles_dict[ptype],\n",
    "                                     marker='.',\n",
    "                                     capsize=2,\n",
    "                                     label=f'CCL: {alpha}={alphai}°')\n",
    "                ax2_aois.set_ylabel('CCL [Å]')  \n",
    "                ax2_aois.set_ylim((5,40))\n",
    "                \n",
    "#             if ptype == 'sums' and aoi == 'th0.080':\n",
    "#             # if ptype == 'sums':\n",
    "#                 if ax3_aois is None:\n",
    "#                     ax3_aois = ax1_aois.twinx()\n",
    "#                     ax3_aois.spines['right'].set_position(('outward', 45))  # Offset the third axis\n",
    "                    \n",
    "#                 ax3_aois.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "#                                      avgs, \n",
    "#                                      yerr=errs, \n",
    "#                                      color=sums_colors_dict[aoi],\n",
    "#                                      linestyle=linestyles_dict[ptype],\n",
    "#                                      marker='.',\n",
    "#                                      capsize=2,\n",
    "#                                      label=f'rDoC: {alpha}={alphai}°')\n",
    "#                 ax3_aois.set_ylabel('Peak sum [a.u.]')  \n",
    "            \n",
    "            \n",
    "\n",
    "        fig_aois.legend(loc='upper right', bbox_to_anchor=[3,0.95])\n",
    "        fig_aois.suptitle(f'PM6 {solvent} average {peak} fit parameters', y=0.98)\n",
    "\n",
    "        # # Save\n",
    "        # savePath = outPath.joinpath('giwaxs_plots/linefit_summaries_v3')\n",
    "        # fig_aois.savefig(savePath.joinpath(f'PM6_{solvent}_{peak}_aois.png'), dpi=150, bbox_inches='tight')\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "        plt.close('all')  \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd19d9-fb62-43c4-818d-d2499c470d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_chis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e7bc4-cdec-4829-8059-51a97b0fe93d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "0.5*(3*np.cos(np.deg2rad(avg_chis))**2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f40bd-ccde-44dd-b33e-089cdb4d7126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409282a9-4f73-48d8-b69e-6d47420ae469",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict = {\n",
    "    \n",
    "}\n",
    "linestyles_dict = {}\n",
    "        fig_aois, ax1_aois = plt.subplots(figsize=(3.5,2.5), dpi=150, tight_layout=True)\n",
    "        ax1_aois.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "                             avg_chis, \n",
    "                             yerr=avg_chi_errs, \n",
    "                             color=param_colors[0],\n",
    "                             marker='o',\n",
    "                             capsize=4,\n",
    "                             label='Stacking angle')\n",
    "        ax1_aois.set_ylabel('Stacking angle χ [°]')\n",
    "        ax1_aois.set_xlabel('CN additive [vol %]')\n",
    "\n",
    "        if peak == 'lamella':\n",
    "            ax2_aois = ax1_summary.twinx()\n",
    "            ax2_aois.errorbar([label.split()[0][:-1] for label in additive_label], \n",
    "                                 avg_ccls, \n",
    "                                 yerr=avg_ccl_errs, \n",
    "                                 color=param_colors[1],\n",
    "                                 marker='o',\n",
    "                                 capsize=4,\n",
    "                                 label='CCL')\n",
    "            ax2_aois.set_ylabel('CCL [Å]')\n",
    "\n",
    "        fig_summary.legend(loc='upper right', bbox_to_anchor=[1.45,0.62])\n",
    "        fig_summary.suptitle(f'PM6 {solvent}, {alpha}={alphai}°\\n Average {peak} fit parameters', y=0.9)\n",
    "\n",
    "        # # Save\n",
    "        # savePath = outPath.joinpath('giwaxs_plots/linefit_summaries_v1')\n",
    "        # fig_summary.savefig(savePath.joinpath(f'PM6_{solvent}_{peak}_{incident_angle}_summary.png'), dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531acacd-27aa-4b15-961a-2c84157493bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(np.pi*2)/3.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864d347-673a-4333-b24d-b9b1d6f1f94a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load all_params object from saved folders, but only do so if \n",
    "# bkg_frac = bkgFracPath.name.split('_')[-1].split('-')[-1]  # extract background fraction from folder name\n",
    "\n",
    "# all_params = {}\n",
    "# for measurementPath in bkgFracPath.glob('PM6*'):\n",
    "#     measurement_name = measurementPath.name  # get fit name\n",
    "    \n",
    "#     params = {}\n",
    "    \n",
    "#     # Load lmfit results\n",
    "#     for saved_result in measurementPath.glob('*.json'):\n",
    "#         result_name = saved_result.stem  # stem doesn't include filetype format at end\n",
    "#         chi_bin_key = result_name.split('_')[-1]\n",
    "        \n",
    "#         result = load_modelresult(saved_result)\n",
    "        \n",
    "#         #only load for chi bins with good fit parameters (low reduced chi and residual average close to zero)\n",
    "#         if result.redchi<100 and np.abs(result.residual.mean())<1:\n",
    "#             params[chi_bin_key] = result.params.valuesdict()\n",
    "        \n",
    "#     all_params[measurement_name] = params\n",
    "    \n",
    "# # all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0a53f-2d6b-4384-bce9-4440dea564ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Previous code used for fitting peak results extracted from 'all_params' dictionaries\n",
    "# # The 'all_params dict' goes sample/scan key, chi_bin key, parameters dict\n",
    "\n",
    "# # Plot pseudovoigt fractions, d-spacings, scherrer coherence lengths, and normalized peak areas vs chi\n",
    "# %matplotlib inline\n",
    "# plt.close('all')\n",
    "\n",
    "# # Initialize figures to be populated later\n",
    "# fig_psvoigt, ax_psvoigt = plt.subplots(figsize=((5.5,2.5)), dpi=150, tight_layout=True)\n",
    "# fig_dspacing, ax_dspacing = plt.subplots(figsize=((5.5,2.5)), dpi=150, tight_layout=True)\n",
    "# fig_ccl, ax_ccl = plt.subplots(figsize=((5.5,2.5)), dpi=150, tight_layout=True)\n",
    "# fig_pole, ax_pole = plt.subplots(figsize=((5.5,2.5)), dpi=150, tight_layout=True)\n",
    "# # fig_pole, ax_pole = plt.subplots(figsize=((2.8,3.3)), dpi=150, tight_layout=True)\n",
    "\n",
    "# # # Choose sample sets (by folder/sample name):\n",
    "# peak = '0p9'\n",
    "# # incident_angle = 'th0.110'\n",
    "# # PM6_CF_samples = [f'PM6_0CN-CF_x0.000_{incident_angle}', f'PM6_p5CN-CF_x0.000_{incident_angle}', f'PM6_1CN-CF_x2.000_{incident_angle}', f'PM6_5CN-CF_x2.000_{incident_angle}']\n",
    "# # PM6_CB_samples = [f'PM6_0CN-CB_x0.000_{incident_angle}', f'PM6_p5CN-CB_x0.000_{incident_angle}', f'PM6_1CN-CB_x0.000_{incident_angle}', f'PM6_5CN-CB_x0.000_{incident_angle}']\n",
    "\n",
    "\n",
    "# # Define custom colors for each film, here I've chosen just two shades along the same sequention colorbar for each sample group\n",
    "# # colors = plt.cm.viridis_r(np.linspace(0,1,len(PM6_CF_samples)))\n",
    "# colors = plt.cm.viridis_r(np.linspace(0,1,3))\n",
    "\n",
    "# ### Extracting info from all_params & populating ___ vs chi plots:\n",
    "# summed_peak_areas = {}\n",
    "# avg_dspacings = {}\n",
    "# avg_peak_centers = {}\n",
    "# avg_coherence_lengths = {}\n",
    "# for i, film in enumerate(all_params.keys()):\n",
    "# # for i, film in enumerate(PM6_CF_samples):\n",
    "# # for i, film in enumerate(PM6_CB_samples):\n",
    "#     peak_areas = []\n",
    "#     pvoigt_fracs = []\n",
    "#     peak_centers = []\n",
    "#     peak_fwhms = []\n",
    "    \n",
    "#     chis = []\n",
    "#     for chi_bin, params in all_params[film].items():   \n",
    "#         # Get starting value of chi bin\n",
    "#         chi = int(chi_bin.split('-')[0]) + 2  # set to midpoint of values\n",
    "#         chis.append(chi)\n",
    "#         # Get peak areas\n",
    "#         peak_area = params[f'amp_{peak}']\n",
    "#         peak_areas.append(peak_area)\n",
    "#         # Get pseudovoigt fraction\n",
    "#         pvoigt_frac = params[f'fraction_{peak}']\n",
    "#         pvoigt_fracs.append(pvoigt_frac)\n",
    "#         # Get peak centers\n",
    "#         peak_center = params[f'cen_{peak}']\n",
    "#         peak_centers.append(peak_center)\n",
    "#         # Get peak fwhms\n",
    "#         peak_fwhm = params[f'sigma_{peak}'] * 2\n",
    "#         peak_fwhms.append(peak_fwhm)\n",
    "\n",
    "#     # Get normalized peak areas (divide by sum of all peak areas; adds to 1) and write area sum to dict\n",
    "#     peak_areas = np.array(peak_areas)\n",
    "#     summed_peak_areas[film] = np.sum(peak_areas)  # Write sum of peak area out, this corresponds to relative extent of crystallinity if thickness normalized\n",
    "#     normed_peak_areas = peak_areas / np.sum(peak_areas)\n",
    "\n",
    "#     # Calculate d-spacings from peak centers, calculate peak-area-weighted average of d-spacing\n",
    "#     dspacings = (2*np.pi) / np.array(peak_centers) \n",
    "#     avg_peak_centers[film] = peak_center\n",
    "#     avg_dspacing = np.round(np.sum(dspacings*normed_peak_areas), 2)\n",
    "#     avg_dspacings[film] = avg_dspacing  \n",
    "\n",
    "#     # Calculate coherence length from peak fwhm, calculate peak-area-weighted average of coherence length    \n",
    "#     coherence_lengths = (2*np.pi*0.9) / np.array(peak_fwhms) \n",
    "#     avg_coherence_length = np.round(np.sum(coherence_lengths*normed_peak_areas), 2)\n",
    "#     avg_coherence_lengths[film] = avg_coherence_length\n",
    "\n",
    "#     ## Plotting\n",
    "#     # Plot pseudovoigt fraction vs chi for each film\n",
    "#     ax_psvoigt.errorbar(chis, pvoigt_fracs, label=film, color=colors[i], marker='o')\n",
    "#     ax_psvoigt.set(title=f'{peak} pseudo-voigt lorentzian-to-gaussian ratio versus $\\\\chi$',\n",
    "#                    ylabel='Lorentzian peak fraction',\n",
    "#                    xlabel='Binned $\\\\chi$ value [°]')\n",
    "#     ax_psvoigt.legend(title='Film', loc='upper left', bbox_to_anchor=(1,1))\n",
    "\n",
    "#     # Plot d-spacing vs chi for each film\n",
    "#     ax_dspacing.errorbar(chis, dspacings, label=film, color=colors[i], marker='o', \n",
    "#                          capsize=4)\n",
    "#     ax_dspacing.set(title=f'{peak} d-spacing versus $\\\\chi$',\n",
    "#                     ylabel='d-spacing [Å]',\n",
    "#                     xlabel='$\\\\chi$ value [°]')\n",
    "#                     # ylim=(3.55,3.8))\n",
    "#     ax_dspacing.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.85))\n",
    "#     ax_dspacing.grid(visible=True,which='major',axis='y')\n",
    "\n",
    "#     # Plot coherence length vs chi for each film\n",
    "#     ax_ccl.errorbar(chis, coherence_lengths, label=film, color=colors[i], marker='o', \n",
    "#                     capsize=4)\n",
    "#     ax_ccl.set(title=f'{peak} crystalline coherence length (CCL) versus $\\\\chi$',\n",
    "#                ylabel='CCL [Å]',\n",
    "#                xlabel='$\\\\chi$ value [°]')\n",
    "#                # ylim=(13,18.5))\n",
    "#     ax_ccl.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.85))\n",
    "#     ax_ccl.grid(visible=True,which='major',axis='y')\n",
    "    \n",
    "#     # Plot 'pole figure' for each film\n",
    "#     ax_pole.errorbar(chis, normed_peak_areas, label=film, color=colors[i], marker='o',\n",
    "#                      capsize=4)\n",
    "#     ax_pole.set(title=f'{peak} pole figure',\n",
    "#                 ylabel='Peak area [arb. units]',\n",
    "#                 xlabel='$\\\\chi$ value [°]')\n",
    "#     # ax_pole.set_ybound(lower=0, upper=0.21)\n",
    "#     ax_pole.grid(visible=True,which='major',axis='y')\n",
    "#     # ax_pole.legend(title='Film')    \n",
    "#     ax_pole.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.85))\n",
    "\n",
    "#     # Extract chi values for file saving purposes\n",
    "#     chi_width = chis[0]-chis[1]\n",
    "#     chi_bins = len(chis)\n",
    "#     chi_min = chis[-1] - 1\n",
    "#     chi_max = chis[0]+chi_width - 1\n",
    "\n",
    "#     # # Save\n",
    "#     # parent_folder = 'full_chi_fit_results_plots_v1'\n",
    "#     # outPath.joinpath(parent_folder).mkdir(exist_ok=True)\n",
    "#     # savePath = outPath.joinpath(parent_folder, f'chiWidth-{chi_width}_chiBins-{chi_bins}_chiRange{chi_min}-{chi_max}')\n",
    "#     # savePath.mkdir(exist_ok=True)\n",
    "#     # fig_psvoigt.savefig(savePath.joinpath(f'pseudovoigt_ratios_pi-pi_fit.png'), dpi=150)\n",
    "#     # fig_dspacing.savefig(savePath.joinpath(f'dspacings_pi-pi_fit.png'), dpi=150)\n",
    "#     # fig_ccl.savefig(savePath.joinpath(f'coherence_lengths_pi-pi_fit.png'), dpi=150)\n",
    "#     # fig_pole.savefig(savePath.joinpath(f'peak_areas_pi-pi_fit.png'), dpi=150)\n",
    "#     # # fig_pole.savefig(savePath.joinpath(f'peak_areas_pi-pi_fit_narrow.png'), dpi=150)\n",
    "    \n",
    "# plt.show()\n",
    "# plt.close('all')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d8934a-ef80-499a-898d-adf87dbeb65b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Unused code for using residual + minimize fitting (same as above but less useful result objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bda624-c60b-4efe-b699-c6c99248bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for if using a residual function and lmfit.minimize \n",
    "def linear_pars(pars, x):\n",
    "    slope_lin, intercept_lin = pars[f'slope_lin'].value, pars[f'intercept_lin'].value\n",
    "    return linear(x, slope_lin, intercept_lin)\n",
    "\n",
    "def exponential_pars(pars, x):\n",
    "    amp_exp, decay_exp = pars[f'amp_exp'].value, pars[f'decay_exp'].value\n",
    "    return exponential(x, amp_exp, decay_exp)\n",
    "\n",
    "def pvoigt_peak(pars, x, peakname):\n",
    "    amp, cen, sigma, fraction = pars[f'amp_{peakname}'].value, pars[f'cen_{peakname}'].value, pars[f'sigma_{peakname}'].value, pars[f'fraction_{peakname}'].value\n",
    "    return pvoigt(x, amp, cen, sigma, fraction)\n",
    "        \n",
    "# Define residiual function to minimize for a fit of an individual linecut:\n",
    "def residual(pars, x, data):\n",
    "    model = total_func(x, \n",
    "                       pars['slope_lin'].value, pars['intercept_lin'].value,\n",
    "                       pars['amp_exp'].value, pars['decay_exp'].value,\n",
    "                       pars['amp_lamella'].value, pars['cen_lamella'].value, pars['sigma_lamella'].value, pars['fraction_lamella'].value,\n",
    "                       pars['amp_backbone'].value, pars['cen_backbone'].value, pars['sigma_backbone'].value, pars['fraction_backbone'].value,\n",
    "                       pars['amp_0p9'].value, pars['cen_0p9'].value, pars['sigma_0p9'].value, pars['fraction_0p9'].value,\n",
    "                       pars['amp_1p2'].value, pars['cen_1p2'].value, pars['sigma_1p2'].value, pars['fraction_1p2'].value,\n",
    "                       pars['amp_alkyl'].value, pars['cen_alkyl'].value, pars['sigma_alkyl'].value, pars['fraction_alkyl'].value,\n",
    "                       pars['amp_pipi'].value, pars['cen_pipi'].value, pars['sigma_pipi'].value, pars['fraction_pipi'].value)\n",
    "    return model - data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ec330-ede2-4378-a0f6-25fa5afd5763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select data linecut, create parameters, run fit:\n",
    "# This cell is working for individual linecuts, using residual function syntax\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "### Select data\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "# chi_min, chi_width, chi_bins = [10, 6, 12]  \n",
    "chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "q_min = 0.25  # for full qr cut\n",
    "q_max = 1.78\n",
    "\n",
    "# Select attribute\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22', 'AL25', 'AL26'], 'incident_angle': ['th0.110']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "all_outs = {}\n",
    "good_ratios = {}\n",
    "bad_ratios = {}\n",
    "for DA in tqdm(selected_DAs):\n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "    binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi')\n",
    "    \n",
    "    out = None  # clear any default parameters from previous sample\n",
    "    ratios = {}\n",
    "    for i, chi_bin in tqdm(enumerate(binned_DA.chi_bins.data), total=len(binned_DA.chi_bins.data)):\n",
    "        sel_DA = binned_DA.sel(chi_bins=chi_bin)  \n",
    "\n",
    "        x = sel_DA.qr.data\n",
    "        data = sel_DA.data\n",
    "        \n",
    "        bkg_x = 0.56\n",
    "        bkg_frac = 0.4\n",
    "        bkg_val = float(sel_DA.sel(qr=slice(bkg_x-0.01, bkg_x+0.01)).mean('qr')) * bkg_frac\n",
    "        print(f'bkg value: {bkg_val}')\n",
    "            \n",
    "        #  Initialize parameter values & constraints:\n",
    "        if out and out.redchi<100:\n",
    "            params = out.params\n",
    "            params.set(intercept_lin = {'value':bkg_val, 'min':bkg_val*0.85, 'max':bkg_val*1.15})\n",
    "            if chi_bin.left<10:\n",
    "                params.set(amp_exp = {'value':0, 'vary':False})\n",
    "            if chi_bin.left<50:\n",
    "                params.set(amp_backbone = {'value':0, 'vary':False})\n",
    "        else:\n",
    "            params = Parameters()   \n",
    "            params.add(f'slope_lin',         value=0,     vary=False)\n",
    "            params.add(f'intercept_lin',     value=bkg_val, min=bkg_val*0.9, max=bkg_val*1.1)\n",
    "            if chi_bin.left<10:\n",
    "                params.add(f'amp_exp',           value=10,      min=0.01,  max=1e2, vary=False)\n",
    "                params.add(f'decay_exp',         value=0.1,     min=0.01,  max=1e2, vary=False)\n",
    "            else:\n",
    "                params.add(f'amp_exp',           value=0,     vary=False)\n",
    "                params.add(f'decay_exp',         value=0,     vary=False)\n",
    "            params.add(f'amp_lamella',       value=250,   min=0,     max=1e4)\n",
    "            params.add(f'cen_lamella',       value=0.31,  min=0.28,  max=0.34)\n",
    "            params.add(f'sigma_lamella',     value=0.05,  min=0.01,  max=0.2)\n",
    "            params.add(f'fraction_lamella',  value=0.5,   min=0,     max=1)\n",
    "            if chi_bin.left<50:\n",
    "                params.add(f'amp_backbone',      value=0, vary=False)\n",
    "            else:\n",
    "                params.add(f'amp_backbone',      value=0.005, min=0,     max=10)\n",
    "            params.add(f'cen_backbone',      value=0.66,  min=0.63,  max=0.7)\n",
    "            params.add(f'sigma_backbone',    value=1e-3,  min=0.01,  max=0.05)\n",
    "            params.add(f'fraction_backbone', value=0.5,   min=0,     max=1)\n",
    "            params.add(f'amp_0p9',           value=1,     min=0,     max=10)\n",
    "            params.add(f'cen_0p9',           value=0.93,  min=0.85,  max=0.99)\n",
    "            params.add(f'sigma_0p9',         value=0.02,  min=0.01,  max=0.4)\n",
    "            params.add(f'fraction_0p9',      value=0.5,   min=0,     max=1)\n",
    "            params.add(f'amp_1p2',           value=1,     min=0,     max=10)\n",
    "            params.add(f'cen_1p2',           value=1.2,   min=1.15,  max=1.25)\n",
    "            params.add(f'sigma_1p2',         value=0.5,   min=0.05,  max=0.5)\n",
    "            params.add(f'fraction_1p2',      value=0.5,   min=0,     max=1)\n",
    "            params.add(f'amp_alkyl',         value=50,    min=0,     max=400)\n",
    "            params.add(f'cen_alkyl',         value=1.4,   min=1.35,  max=1.45)\n",
    "            params.add(f'sigma_alkyl',       value=0.5,   min=0.1,   max=0.5)\n",
    "            params.add(f'fraction_alkyl',    value=0.5,   min=0,     max=1)\n",
    "            params.add(f'amp_pipi',          value=100,   min=0,     max=1000)\n",
    "            params.add(f'cen_pipi',          value=1.73,  min=1.65,  max=1.8)\n",
    "            params.add(f'sigma_pipi',        value=0.1,   min=0.05,  max=0.3)\n",
    "            params.add(f'fraction_pipi',     value=0.5,   min=0,     max=1)    \n",
    "\n",
    "        ### Run minimization(s):\n",
    "        out = minimize(residual, params, args=(x,data))\n",
    "        \n",
    "        # Retry up to some limit number of times for better fit result\n",
    "        limit = 10\n",
    "        counter = 0\n",
    "        while out.redchi>50:\n",
    "            # bad fit, retry\n",
    "            out = minimize(residual, out.params, args=(x,data))\n",
    "            counter += 1\n",
    "            if counter >= limit:\n",
    "                break            \n",
    "        \n",
    "        print(f'Retried fit {counter} times...')\n",
    "                \n",
    "        full_fit = data + out.residual\n",
    "        \n",
    "        all_outs[f'{sn_id[binned_DA.sample_ID]}_{binned_DA.incident_angle}'] = out\n",
    "\n",
    "\n",
    "        # Plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        lin_bkg = linear_pars(out.params, x)\n",
    "        exp_bkg = exponential_pars(out.params, x)\n",
    "        pvoigt1 = pvoigt_peak(out.params, x, 'lamella')\n",
    "        pvoigt2 = pvoigt_peak(out.params, x, 'backbone')\n",
    "        pvoigt3 = pvoigt_peak(out.params, x, '0p9')\n",
    "        pvoigt4 = pvoigt_peak(out.params, x, '1p2')\n",
    "        pvoigt5 = pvoigt_peak(out.params, x, 'alkyl')\n",
    "        pvoigt6 = pvoigt_peak(out.params, x, 'pipi')\n",
    "\n",
    "        sig_lamella = out.params['sigma_lamella'].value\n",
    "        sig_backbone = out.params['sigma_backbone'].value\n",
    "        sig_0p9 = out.params['sigma_0p9'].value\n",
    "        sig_1p2 = out.params['sigma_1p2'].value\n",
    "        sig_alkyl = out.params['sigma_alkyl'].value\n",
    "        sig_pipi = out.params['sigma_pipi'].value\n",
    "              \n",
    "        # ratios[f'lamella-pipi_sig_{round(chi_bin.left)}-{round(chi_bin.right)}'] = np.round(sig_lamella/sig_pipi, 2)\n",
    "        # ratios[f'lamella-alkyl_sig_{round(chi_bin.left)}-{round(chi_bin.right)}'] = np.round(sig_lamella/sig_alkyl, 2)\n",
    "        # if out.redchi<50 and np.abs(out.residual.mean())<1:\n",
    "        #     good_ratios[f'{sn_id[binned_DA.sample_ID]}_{binned_DA.incident_angle}'] = ratios\n",
    "        #     print(f'lamella-pipi sig ratio: {np.round(sig_lamella/sig_pipi, 2)}')\n",
    "        #     print(f'lamella-alkyl sig ratio: {np.round(sig_lamella/sig_alkyl, 2)}')\n",
    "        # else:\n",
    "        #     print('bad fit, so dont compare these ratios')\n",
    "        #     bad_ratios[f'{sn_id[binned_DA.sample_ID]}_{binned_DA.incident_angle}'] = ratios\n",
    "\n",
    "        ax.plot(x, data, 'o')\n",
    "        ax.plot(x, full_fit, '-', label='full_fit')\n",
    "        ax.plot(x, lin_bkg, label=f'lin_bkg')\n",
    "        ax.plot(x, exp_bkg, label=f'exp_bkg')\n",
    "        ax.plot(x, pvoigt1, label=f'lamella: sig={sig_lamella}')\n",
    "        ax.plot(x, pvoigt2, label=f'backbone: sig={sig_backbone}')\n",
    "        ax.plot(x, pvoigt3, label=f'0p9: sig={sig_0p9}')\n",
    "        ax.plot(x, pvoigt4, label=f'1p2: sig={sig_1p2}')\n",
    "        ax.plot(x, pvoigt5, label=f'alkyl: sig={sig_alkyl}')\n",
    "        ax.plot(x, pvoigt6, label=f'pipi: sig={sig_pipi}')\n",
    "        ax.set_title(f'{sn[sel_DA.sample_ID]}_{chi_bin}')\n",
    "        ax.legend(title=f'redchi={np.round(out.redchi, 1)}, abs_resid_mean={np.round(np.abs(out.residual).mean(), 1)}, resid_mean={np.round(out.residual.mean(), 1)}')\n",
    "\n",
    "        plt.show()\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbafd8a-273f-4660-b06f-eb67ff66b7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lamella_alkyl_ratios = []\n",
    "# lamella_pipi_ratios = []\n",
    "# for sample, ratios in good_ratios.items():\n",
    "#     for ratio_chi, value in ratios.items():\n",
    "#         ratio, sig, chi_bin = ratio_chi.split('_')\n",
    "#         if ratio=='lamella-alkyl':\n",
    "#             lamella_alkyl_ratios.append(value)\n",
    "#         elif ratio=='lamella-pipi':\n",
    "#             lamella_pipi_ratios.append(value)\n",
    "            \n",
    "# lamella_pipi_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3cf2e-703d-4fce-9896-396f201bf2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lamella_alkyl_ratios = []\n",
    "# lamella_pipi_ratios = []\n",
    "# for sample, ratios in bad_ratios.items():\n",
    "#     for ratio_chi, value in ratios.items():\n",
    "#         ratio, sig, chi_bin = ratio_chi.split('_')\n",
    "#         if ratio=='lamella-alkyl':\n",
    "#             lamella_alkyl_ratios.append(value)\n",
    "#         elif ratio=='lamella-pipi':\n",
    "#             lamella_pipi_ratios.append(value)\n",
    "            \n",
    "# lamella_pipi_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fdc9b-4bbb-40b8-8775-3d300e69da38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Unused code for simultaneous fitting (slow, inefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c7f2b-3c8a-4b6a-beb9-93f99af74e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that will loop over a dataset for fitting multiple lines simulataneously:\n",
    "def linear_dataset(pars, x, i):\n",
    "    slope_lin, intercept_lin = pars[f'slope_lin_{i}'].value, pars[f'intercept_lin_{i}'].value\n",
    "    return linear(x, slope_lin, intercept_lin)\n",
    "\n",
    "def exponential_dataset(pars, x, i):\n",
    "    amp_exp, decay_exp = pars[f'amp_exp_{i}'].value, pars[f'decay_exp_{i}'].value\n",
    "    return exponential(x, amp_exp, decay_exp)\n",
    "\n",
    "def pvoigt_dataset(pars, x, i, peakname):\n",
    "    amp, cen, sigma, fraction = pars[f'amp_{peakname}_{i}'].value, pars[f'cen_{peakname}_{i}'].value, pars[f'sigma_{peakname}_{i}'].value, pars[f'fraction_{peakname}_{i}'].value\n",
    "    return pvoigt(x, amp, cen, sigma, fraction)\n",
    "    \n",
    "def total_func_dataset(pars, x, i):\n",
    "    slope_lin, intercept_lin = pars[f'slope_lin_{i}'].value, pars[f'intercept_lin_{i}'].value\n",
    "    amp_exp, decay_exp = pars[f'amp_exp_{i}'].value, pars[f'decay_exp_{i}'].value\n",
    "    amp_lamella, cen_lamella, sigma_lamella, fraction_lamella = pars[f'amp_lamella_{i}'].value, pars[f'cen_lamella_{i}'].value, pars[f'sigma_lamella_{i}'].value, pars[f'fraction_lamella_{i}'].value\n",
    "    amp_backbone, cen_backbone, sigma_backbone, fraction_backbone = pars[f'amp_backbone_{i}'].value, pars[f'cen_backbone_{i}'].value, pars[f'sigma_backbone_{i}'].value, pars[f'fraction_backbone_{i}'].value\n",
    "    amp_0p9, cen_0p9, sigma_0p9, fraction_0p9 = pars[f'amp_0p9_{i}'].value, pars[f'cen_0p9_{i}'].value, pars[f'sigma_0p9_{i}'].value, pars[f'fraction_0p9_{i}'].value\n",
    "    amp_1p2, cen_1p2, sigma_1p2, fraction_1p2 = pars[f'amp_1p2_{i}'].value, pars[f'cen_1p2_{i}'].value, pars[f'sigma_1p2_{i}'].value, pars[f'fraction_1p2_{i}'].value\n",
    "    amp_alkyl, cen_alkyl, sigma_alkyl, fraction_alkyl = pars[f'amp_alkyl_{i}'].value, pars[f'cen_alkyl_{i}'].value, pars[f'sigma_alkyl_{i}'].value, pars[f'fraction_alkyl_{i}'].value\n",
    "    amp_pipi, cen_pipi, sigma_pipi, fraction_pipi = pars[f'amp_pipi_{i}'].value, pars[f'cen_pipi_{i}'].value, pars[f'sigma_pipi_{i}'].value, pars[f'fraction_pipi_{i}'].value  \n",
    "    \n",
    "    return total_func(x, \n",
    "                      slope_lin, intercept_lin,\n",
    "                      amp_exp, decay_exp,\n",
    "                      amp_lamella, cen_lamella, sigma_lamella, fraction_lamella,\n",
    "                      amp_backbone, cen_backbone, sigma_backbone, fraction_backbone,\n",
    "                      amp_0p9, cen_0p9, sigma_0p9, fraction_0p9,\n",
    "                      amp_1p2, cen_1p2, sigma_1p2, fraction_1p2,\n",
    "                      amp_alkyl, cen_alkyl, sigma_alkyl, fraction_alkyl,\n",
    "                      amp_pipi, cen_pipi, sigma_pipi, fraction_pipi)\n",
    "    \n",
    "    \n",
    "def objective(pars, x, data):\n",
    "    \"\"\"Calculate total residual for fits of Gaussians to several data sets.\"\"\"\n",
    "    ndata, _ = data.shape\n",
    "    resid = 0.0*data[:]\n",
    "\n",
    "    # make residual per data set\n",
    "    for i in range(ndata):\n",
    "        resid[i, :] = data[i, :] - total_func_dataset(pars, x, i)\n",
    "\n",
    "    # now flatten this to a 1D array, as minimize() needs\n",
    "    return resid.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57791c-d2fb-45b0-8fbf-3e0f2959e93e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select data linecut, create parameters, run fit:\n",
    "# Goal of this cell to simultaneously fit parameters for all linecuts\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "### Select data\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "# chi_min, chi_width, chi_bins = [10, 6, 12]  \n",
    "chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "q_min = 0.25  # for full qr cut\n",
    "q_max = 1.75\n",
    "\n",
    "# Select attribute\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22', 'AL25', 'AL26'], 'incident_angle': ['th0.110']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "all_outs = {}\n",
    "for DA in tqdm(selected_DAs):\n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "    binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi')\n",
    "    \n",
    "    x = binned_DA.qr.data\n",
    "    data = binned_DA.data\n",
    "    \n",
    "    params = Parameters()   \n",
    "    total_slices = 5\n",
    "    for iy, y in enumerate(data[:total_slices]):\n",
    "        ### Create initial parameter values & set constraints:\n",
    "        params.add(f'slope_lin_{iy}',         value=0,     vary=False)\n",
    "        params.add(f'intercept_lin_{iy}',     value=y.min(), min=y.min()/2, max=y.min()*2)\n",
    "        params.add(f'amp_exp_{iy}',           value=0,     min=0,     max=1e2, vary=False)\n",
    "        params.add(f'decay_exp_{iy}',         value=0,     min=0.01,  max=1e2, vary=False)\n",
    "        params.add(f'amp_lamella_{iy}',       value=250,   min=0,     max=1e4)\n",
    "        params.add(f'cen_lamella_{iy}',       value=0.31,  min=0.28,  max=0.34)\n",
    "        params.add(f'sigma_lamella_{iy}',     value=0.05,  min=1e-5,  max=0.5)\n",
    "        params.add(f'fraction_lamella_{iy}',  value=0.5,   min=0,     max=1)\n",
    "        params.add(f'amp_backbone_{iy}',      value=0.005, min=0,     max=10)\n",
    "        params.add(f'cen_backbone_{iy}',      value=0.66,  min=0.63,  max=0.7)\n",
    "        params.add(f'sigma_backbone_{iy}',    value=1e-3,  min=1e-5,  max=0.4)\n",
    "        params.add(f'fraction_backbone_{iy}', value=0.5,   min=0,     max=1)\n",
    "        params.add(f'amp_0p9_{iy}',           value=1,     min=0,     max=10)\n",
    "        params.add(f'cen_0p9_{iy}',           value=0.93,  min=0.85,  max=0.99)\n",
    "        params.add(f'sigma_0p9_{iy}',         value=0.02,  min=1e-5,  max=0.4)\n",
    "        params.add(f'fraction_0p9_{iy}',      value=0.5,   min=0,     max=1)\n",
    "        params.add(f'amp_1p2_{iy}',           value=1,     min=0,     max=10)\n",
    "        params.add(f'cen_1p2_{iy}',           value=1.2,   min=1.15,  max=1.25)\n",
    "        params.add(f'sigma_1p2_{iy}',         value=0.5,   min=0,     max=1)\n",
    "        params.add(f'fraction_1p2_{iy}',      value=0.5,   min=0,     max=1)\n",
    "        params.add(f'amp_alkyl_{iy}',         value=50,    min=0,     max=400)\n",
    "        params.add(f'cen_alkyl_{iy}',         value=1.4,   min=1.35,  max=1.45)\n",
    "        params.add(f'sigma_alkyl_{iy}',       value=0.5,   min=0,     max=1)\n",
    "        params.add(f'fraction_alkyl_{iy}',    value=0.5,   min=0,     max=1)\n",
    "        params.add(f'amp_pipi_{iy}',          value=100,   min=0,     max=1000)\n",
    "        params.add(f'cen_pipi_{iy}',          value=1.73,  min=1.65,  max=1.8)\n",
    "        params.add(f'sigma_pipi_{iy}',        value=0.1,   min=0,     max=1)\n",
    "        params.add(f'fraction_pipi_{iy}',     value=0.5,   min=0,     max=1)    \n",
    "        \n",
    "    out = minimize(objective, params, args=(x, data[:total_slices]))\n",
    "    all_outs[f'{sn_id[binned_DA.sample_ID]}_{binned_DA.incident_angle}'] = out\n",
    "    \n",
    "    for iy, y in enumerate(data[:total_slices]):\n",
    "        fig, ax = plt.subplots()\n",
    "        full_fit = total_func_dataset(out.params, x, iy)\n",
    "        lin_bkg = linear_dataset(out.params, x, iy)\n",
    "        exp_bkg = exponential_dataset(out.params, x, iy)\n",
    "        pvoigt1 = pvoigt_dataset(out.params, x, iy, 'lamella')\n",
    "        pvoigt2 = pvoigt_dataset(out.params, x, iy, 'backbone')\n",
    "        pvoigt3 = pvoigt_dataset(out.params, x, iy, '0p9')\n",
    "        pvoigt4 = pvoigt_dataset(out.params, x, iy, '1p2')\n",
    "        pvoigt5 = pvoigt_dataset(out.params, x, iy, 'alkyl')\n",
    "        pvoigt6 = pvoigt_dataset(out.params, x, iy, 'pipi')\n",
    "\n",
    "        ax.plot(x, y, 'o')\n",
    "        ax.plot(x, full_fit, '-', label='full_fit')\n",
    "        ax.plot(x, lin_bkg, label='lin_bkg')\n",
    "        ax.plot(x, exp_bkg, label='exp_bkg')\n",
    "        ax.plot(x, pvoigt1, label='lamella')\n",
    "        ax.plot(x, pvoigt2, label='backbone')\n",
    "        ax.plot(x, pvoigt3, label='0p9')\n",
    "        ax.plot(x, pvoigt4, label='1p2')\n",
    "        ax.plot(x, pvoigt5, label='alkyl')\n",
    "        ax.plot(x, pvoigt6, label='pipi')\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48658f68-02b6-43d4-a4b2-8a1dddc60322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215d42e-403f-4bfb-bfcf-92b55332ede8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(total_slices):\n",
    "    intercept = out.params[f'intercept_lin_{i}']\n",
    "    print(intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99301f-339e-4987-b7cf-076583075f80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(total_slices):\n",
    "    amp = out.params[f'amp_{peak}_{i}']\n",
    "    cen = out.params[f'cen_{peak}_{i}']\n",
    "    sigma = out.params[f'sigma_{peak}_{i}']\n",
    "    print(amp)\n",
    "    print(cen)\n",
    "    print(sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b11c82-18e4-468c-a522-2c1110a917e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak = 'pipi'\n",
    "for i in range(total_slices):\n",
    "    amp = out.params[f'amp_{peak}_{i}']\n",
    "    cen = out.params[f'cen_{peak}_{i}']\n",
    "    sigma = out.params[f'sigma_{peak}_{i}']\n",
    "    print(amp)\n",
    "    print(cen)\n",
    "    print(sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33234ad4-14ea-4b68-9e34-def9eb701dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.redchi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86443f31-b6d8-40de-b3fd-a7702cd6dd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3cdea-d639-4c96-bb07-bca868f15973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.abs(out.residual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7627bc-c3d7-41c3-8a5f-906d8f77b555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0990bbe-8bd0-45a1-8883-c72bd14904c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b353f-1cab-4cf5-be29-93f8186ce5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7a1e9-0471-4ae1-9f5b-a54b42a59bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39fad0-e115-4c28-aac6-a9ada3ab76ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8616e3-fbcf-4390-8ef4-03430d202e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5890cd62-5978-46a0-ab47-b7e4ca5c3391",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### PM6 films old fitting functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623bfb7e-9b07-4c75-9681-4aed274434c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_lmfit(sliced_DA):\n",
    "    \"\"\"\n",
    "    Function currently utilizes global variables, define in notebook!\n",
    "    \n",
    "    \"\"\"\n",
    "    # point_x = float(sliced_DA.min())\n",
    "    # point_y = float(sliced_DA.sel(qr=slice(point_x-0.01, point_x+0.01)).mean('qr'))\n",
    "    \n",
    "    # point_y = float(sliced_DA.sel(qr=slice(0.55,0.59)).mean('qr'))\n",
    "    point_y = float(sliced_DA.min())\n",
    "    point_y = point_y - point_y*(0.2)\n",
    "    # point_y = point_y - 100\n",
    "\n",
    "    x = sliced_DA.qr.data\n",
    "    y = sliced_DA.data\n",
    "\n",
    "    # Define all models to include in fitting\n",
    "    bkg_mod = models.LinearModel(prefix='bkg_')\n",
    "    pars = bkg_mod.make_params(intercept=point_y, slope=0)\n",
    "    pars['bkg_intercept'].set(vary=False)\n",
    "    pars['bkg_slope'].set(vary=False)\n",
    "    \n",
    "    exp_mod = models.ExponentialModel(prefix='exp_')\n",
    "    pars += exp_mod.make_params(decay=0.1, amplitude=100)\n",
    "    pars['exp_decay'].set(min=0.01)\n",
    "    pars['exp_amplitude'].set(min=0.1)\n",
    "    \n",
    "    lamella_mod = models.PseudoVoigtModel(prefix='lamella_')  # lamella peak at 0.32\n",
    "    pars += lamella_mod.make_params(center={'value':0.31, 'min':0.28, 'max':0.34}, \n",
    "                                    sigma ={'value':0.09/2, 'min':0.01/2, 'max':0.2/2})\n",
    "    pars['lamella_amplitude'].set(min=30)\n",
    "    \n",
    "    backbone_mod = models.PseudoVoigtModel(prefix='backbone_')  # backbone peak at 0.66\n",
    "    pars += backbone_mod.guess(y, x, center=0.66)\n",
    "    pars['backbone_amplitude'].set(min=0, max=10)\n",
    "    pars['backbone_center'].set(min=0.63, max=0.73)\n",
    "    # pars['backbone_sigma'].set(max=0.1)\n",
    "\n",
    "    pk0p9_mod = models.PseudoVoigtModel(prefix='pk0p9_')  # 300  peak around 0.9\n",
    "    pars += pk0p9_mod.guess(y, x, center=0.93)\n",
    "    pars['pk0p9_amplitude'].set(min=0)\n",
    "    pars['pk0p9_center'].set(min=0.88, max=0.99)\n",
    "\n",
    "    pk1p2_mod = models.PseudoVoigtModel(prefix='pk1p2_')  # 021? peak around 1.2\n",
    "    pars += pk1p2_mod.guess(y, x, center=1.2)\n",
    "    # pars += pk1p2_mod.make_params(center={'value':1.2, 'min':1.15, 'max':1.28}, \n",
    "    #                               sigma ={'value':0.8/2, 'min':0.01/2, 'max':3/2})\n",
    "    pars['pk1p2_amplitude'].set(min=0)\n",
    "    pars['pk1p2_center'].set(min=1.18, max=1.26)\n",
    "    \n",
    "    pkgroup_mod = models.PseudoVoigtModel(prefix='pkgroup_')  # broad collection of slip-stack peaks around 1.2-2\n",
    "    pars += pkgroup_mod.guess(y, x, center=1.4)\n",
    "    # pars += pkgroup_mod.make_params(center={'value':1.4, 'min':1.3, 'max':1.6}, \n",
    "    #                                 sigma ={'value':0.8/2, 'min':0.01/2, 'max':3/2})\n",
    "    pars['pkgroup_center'].set(min=1.3, max=1.5)\n",
    "    pars['pkgroup_amplitude'].set(min=10)\n",
    "    pars['pkgroup_sigma'].set(min=0.1, max=0.4)\n",
    "\n",
    "    pipi_mod = models.PseudoVoigtModel(prefix='pipi_')  # pi-pi peak! around 1.73\n",
    "    pars += pipi_mod.guess(y, x, center=1.73)\n",
    "    pars['pipi_amplitude'].set(min=0, max=500)\n",
    "    pars['pipi_center'].set(min=1.7, max=1.76)\n",
    "    pars['pipi_sigma'].set(max=0.3)\n",
    "    \n",
    "    pk1p86_mod = models.PseudoVoigtModel(prefix='pk1p86_')  # highq peak\n",
    "    pars += pk1p86_mod.guess(y, x, center=1.86)\n",
    "    pars['pk1p86_amplitude'].set(min=0, max=80)\n",
    "    pars['pk1p86_center'].set(min=1.84, max=1.9)\n",
    "\n",
    "    # Combine into full model\n",
    "    mod = bkg_mod + exp_mod + lamella_mod + backbone_mod + pk0p9_mod + pk1p2_mod + pkgroup_mod + pipi_mod + pk1p86_mod\n",
    "\n",
    "    # Run fit and store all info in a ModelResult object\n",
    "    out = mod.fit(y, pars, x=x)\n",
    "    return out\n",
    "\n",
    "def full_fit(sliced_DA, show_plot=True):   \n",
    "    # Run lmfit\n",
    "    out = full_lmfit(sliced_DA)\n",
    "    # FWHM = np.round(float(out.params['pk1_fwhm']), 2)\n",
    "    # Lc = np.round((2*np.pi*0.9)/float(out.params['pk1_fwhm']), 2)\n",
    "    # center = np.round(float(out.params['pk1_center']), 2)\n",
    "    # dspacing = np.round((2*np.pi)/float(out.params['pk1_center']), 2)\n",
    "    \n",
    "    # amplitude = np.round(float(out.params['pk1_amplitude']), 3)\n",
    "    \n",
    "    rsquared = np.round(out.rsquared, 3)\n",
    "    redchi = np.round(out.redchi, 4)\n",
    "\n",
    "    AA1 = '$\\AA^{-1}$'\n",
    "    \n",
    "    # Plot\n",
    "    q = sliced_DA.qr.data\n",
    "    I = sliced_DA.data\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set(size_inches=(6.5,3.5), dpi=120, tight_layout=True)\n",
    "    fig.suptitle(\n",
    "        f'Full fit: {sn[sliced_DA.sample_ID]} {sliced_DA.sample_pos}, $\\\\alpha$={float(sliced_DA.incident_angle[2:])}°; {sliced_DA.chi_bin}° chi',\n",
    "        fontsize=12, y=0.93, x=0.53)\n",
    "       \n",
    "    ax.plot(q, I, label='data', linewidth=2.5)\n",
    "    ax.plot(q, out.best_fit, '--', label='full_fit')\n",
    "    for key in out.eval_components():\n",
    "        ax.plot(q, out.eval_components()[key], label=f'{key}')\n",
    "    ax.set(xlabel=f'Q [{AA1}]', ylabel='Intensity [arb. units]', yscale='linear')\n",
    "    # ax.set_title(\n",
    "    #     f'Center = {center}, FWHM = {FWHM} {AA1} (d-spacing = {dspacing}, $L_c$ = {Lc} $\\AA$)')\n",
    "    ax.legend(title=f'$R^2$ = {rsquared}, $\\chi_r$ = {redchi}', loc='upper left', bbox_to_anchor=(1,1.05))\n",
    "    ax.grid(visible=True, axis='x')\n",
    "        \n",
    "    \n",
    "    # FWHM = np.round(float(out.params['pk1_fwhm']), 2)\n",
    "    # Lc = np.round((2*np.pi*0.9)/FWHM, 2)  # calculate scherrer coherence length\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    return out, fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653867ef-eb74-4518-91ca-18db83c9555e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "# Peak fitting for each chi bin\n",
    "\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "q_min = 0.2\n",
    "q_max = 1.86\n",
    "\n",
    "# Select attributes\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26',\n",
    "                                     'AL31', 'AL41', 'AL28', 'AL38'], \n",
    "                       'incident_angle': ['th0.080']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "# For each selected DA, fit the π-π peak\n",
    "all_outs = {}\n",
    "all_params = {}\n",
    "for DA in tqdm(selected_DAs):\n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "    binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi').compute()\n",
    "    outs = {}\n",
    "    params = {}\n",
    "    for i, chi_bin in tqdm(enumerate(binned_DA.chi_bins.data[::1]), desc='chi_bins', total=len(binned_DA.chi_bins.data)):\n",
    "        key = f'{str(round(chi_bin.left))}-{str(round(chi_bin.right))}'\n",
    "        sel_DA = binned_DA.sel(chi_bins=chi_bin)\n",
    "        sel_DA.attrs['chi_bin'] = chi_bin\n",
    "        outs[key], fig, ax = full_fit(sel_DA)\n",
    "        params[key] = outs[key].params.valuesdict()\n",
    "\n",
    "        # Save paths\n",
    "        outPath.joinpath('full_linefits_v1').mkdir(exist_ok=True)\n",
    "        savePath = outPath.joinpath('full_linefits_v1', f'chiWidth-{chi_width}_chiBins-{chi_bins}_chiRange{chi_min}-{chi_max}')\n",
    "        savePath.mkdir(exist_ok=True)\n",
    "        sampleFitPath = savePath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}')\n",
    "        sampleFitPath.mkdir(exist_ok=True)\n",
    "                                          \n",
    "        # Save plot \n",
    "        fig.savefig(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_chi{key}_pi-pi_fit.png'), dpi=150)\n",
    "\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "                         \n",
    "    # Save fit params:\n",
    "    json_data = json.dumps(params)\n",
    "    with open(str(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_params.json')), 'w') as f:\n",
    "        f.write(json_data)   \n",
    "                                          \n",
    "    # Save fit results\n",
    "    for chi_bin, out in outs.items():\n",
    "        with sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_{chi_bin}_fit_result.txt').open(mode='w') as f:\n",
    "            f.write(out.fit_report())\n",
    "        \n",
    "    # Append to full in-memory dictionaries\n",
    "    all_outs[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}'] = outs\n",
    "    all_params[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}'] = params\n",
    "\n",
    "# # Save model_params.json in save folder for all samples\n",
    "# json_data = json.dumps(all_params)\n",
    "# with open(str(savePath.joinpath('model_params.json')), 'w') as f:\n",
    "#     f.write(json_data)\n",
    "\n",
    "# # Create full fit reports folder and save entire report .txt files inside:\n",
    "# savePath.joinpath('fit_reports').mkdir(exist_ok=True)\n",
    "# for film, outs in all_outs.items():\n",
    "#     for chi_bin, out in outs.items():\n",
    "#         with savePath.joinpath('fit_reports', f'{film}_{chi_bin}_fit_result.txt').open(mode='w') as f:\n",
    "#             f.write(out.fit_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78914b2-55e6-4c9a-ac6f-a83984623cbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### PM6 (GPT) updated fitting functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d9ff2-dc88-462f-af54-164ee255be24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lmfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895c727-9cff-4cd8-8ddc-3ba0c857d226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models.PseudoVoigtModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c490774-b3e0-42a1-8867-478a99787d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_full_model(sliced_DA, prev_params=None):\n",
    "    \"\"\"\n",
    "    Build lmfit model with initial parameter guesses, optionally using previous fit results.\n",
    "    \"\"\"\n",
    "    x = sliced_DA.qr.data\n",
    "    y = sliced_DA.data\n",
    "    \n",
    "    # Define a dictionary for peak models\n",
    "    peak_models = {\n",
    "        'bkg': models.LinearModel(prefix='bkg_'),\n",
    "        'exp': models.ExponentialModel(prefix='exp_'),\n",
    "        'lamella': models.PseudoVoigtModel(prefix='lamella_'),\n",
    "        'backbone': models.PseudoVoigtModel(prefix='backbone_'),\n",
    "        'pk0p9': models.PseudoVoigtModel(prefix='pk0p9_'),\n",
    "        'pk1p2': models.PseudoVoigtModel(prefix='pk1p2_'),\n",
    "        'pkgroup': models.PseudoVoigtModel(prefix='pkgroup_'),\n",
    "        'pipi': models.PseudoVoigtModel(prefix='pipi_'),\n",
    "        # 'pk1p86': models.PseudoVoigtModel(prefix='pk1p86_')\n",
    "    }\n",
    "\n",
    "    # Set up initial parameter guesses (use previous fit parameters if available)\n",
    "    pars = lmfit.Parameters()\n",
    "    \n",
    "    # Add background model\n",
    "    point_y = float(sliced_DA.min()) * 0.02  # Modify the initial y value\n",
    "    pars += peak_models['bkg'].make_params(intercept=point_y, slope=0)\n",
    "    pars['bkg_intercept'].set(vary=False)\n",
    "    pars['bkg_slope'].set(vary=False)\n",
    "\n",
    "    # Add exponential model\n",
    "    pars += peak_models['exp'].make_params(decay=0.1, amplitude=100)\n",
    "    pars['exp_decay'].set(min=0.01)\n",
    "    pars['exp_amplitude'].set(min=0.1)\n",
    "\n",
    "    # Add peaks, updating with previous parameters if available\n",
    "    peak_definitions = [\n",
    "        {'name': 'lamella', \n",
    "         'center': (0.31, 0.28, 0.34), \n",
    "         'amplitude_min': 30},\n",
    "        {'name': 'backbone', \n",
    "         'center': (0.66, 0.63, 0.73), \n",
    "         'amplitude_min': 0, \n",
    "         'amplitude_max': 10},\n",
    "        {'name': 'pk0p9', \n",
    "         'center': (0.93, 0.88, 0.99), \n",
    "         'amplitude_min': 0},\n",
    "        {'name': 'pk1p2', \n",
    "         'center': (1.2, 1.18, 1.22), \n",
    "         'amplitude_min': 0},\n",
    "        {'name': 'pkgroup', \n",
    "         'center': (1.4, 1.35, 1.45), \n",
    "         'amplitude_min': 10},\n",
    "        {'name': 'pipi', \n",
    "         'center': (1.73, 1.7, 1.76), \n",
    "         'amplitude_min': 0, \n",
    "         'amplitude_max': 500},\n",
    "        # {'name': 'pk1p86', 'center': (1.86, 1.84, 1.9), 'amplitude_min': 0, 'amplitude_max': 80}\n",
    "    ]\n",
    "\n",
    "    for peak in peak_definitions:\n",
    "        model_name = peak['name']\n",
    "        model = peak_models[model_name]\n",
    "        center, center_min, center_max = peak['center']\n",
    "        pars += model.guess(y, x, center=center)\n",
    "        pars[f'{model_name}_center'].set(min=center_min, max=center_max)\n",
    "        pars[f'{model_name}_amplitude'].set(min=peak.get('amplitude_min', 0), max=peak.get('amplitude_max', None))\n",
    "        if prev_params:  # Use previous fit results to update initial guesses\n",
    "            pars[f'{model_name}_center'].set(value=prev_params.get(f'{model_name}_center', center))\n",
    "            pars[f'{model_name}_amplitude'].set(value=prev_params.get(f'{model_name}_amplitude', pars[f'{model_name}_amplitude'].value))\n",
    "\n",
    "    # Combine all models\n",
    "    mod = sum([m for name, m in peak_models.items() if name != 'bkg'], peak_models['bkg'])  # Sum all models starting with the background\n",
    "    return mod, pars\n",
    "\n",
    "def plot_fit_results(out, sliced_DA):\n",
    "    \"\"\"\n",
    "    Function to plot fit results and original data.\n",
    "    \"\"\"\n",
    "    q = sliced_DA.qr.data\n",
    "    I = sliced_DA.data\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set(size_inches=(6.5, 3.5), dpi=120, tight_layout=True)\n",
    "    fig.suptitle(f'Fit: {sn[sliced_DA.sample_ID]} {sliced_DA.sample_pos}, '\n",
    "                 f'alpha={float(sliced_DA.incident_angle[2:])}°, {sliced_DA.chi_bin}° chi', fontsize=12, y=0.93, x=0.53)\n",
    "    \n",
    "    ax.plot(q, I, label='Data', linewidth=2.5)\n",
    "    ax.plot(q, out.best_fit, '--', label='Fit')\n",
    "    \n",
    "    # Plot each component\n",
    "    for key, comp in out.eval_components().items():\n",
    "        ax.plot(q, comp, label=f'{key}')\n",
    "    \n",
    "    ax.set(xlabel='Q [$\\AA^{-1}$]', ylabel='Intensity [arb. units]', yscale='linear')\n",
    "    ax.legend(title=f'$R^2$ = {out.rsquared:.3f}, $\\chi_r$ = {out.redchi:.4f}', loc='upper left', bbox_to_anchor=(1, 1.05))\n",
    "    ax.grid(True, axis='x')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def fit_peaks_across_chi_bins(binned_DA, prev_fit_results=None):\n",
    "    \"\"\"\n",
    "    Fit peaks across chi bins, updating initial guesses with the previous fit's results.\n",
    "    \"\"\"\n",
    "    outs = {}\n",
    "    params = {}\n",
    "    prev_params = prev_fit_results or {}\n",
    "    \n",
    "    for i, chi_bin in tqdm(enumerate(binned_DA.chi_bins.data), desc='chi_bins', total=len(binned_DA.chi_bins.data)):\n",
    "        key = f'{round(chi_bin.left)}-{round(chi_bin.right)}'\n",
    "        sel_DA = binned_DA.sel(chi_bins=chi_bin)\n",
    "        sel_DA.attrs['chi_bin'] = chi_bin\n",
    "        \n",
    "        # Build model and run fit using previous parameters\n",
    "        model, pars = build_full_model(sel_DA, prev_params=prev_params)\n",
    "        out = model.fit(sel_DA.data, pars, x=sel_DA.qr.data)\n",
    "        outs[key] = out\n",
    "        \n",
    "        # Store parameters for updating the next bin's initial guesses\n",
    "        prev_params = out.params.valuesdict()\n",
    "        params[key] = prev_params\n",
    "        \n",
    "        # Optionally plot\n",
    "        plot_fit_results(out, sel_DA)\n",
    "\n",
    "    return outs, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44391ec2-1551-47c2-a6bd-41e1d41b6456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "# Full fit for each chi bin\n",
    "\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "# chi_min, chi_width, chi_bins = [10, 8, 9]  \n",
    "chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "q_min = 0.2\n",
    "q_max = 1.82\n",
    "\n",
    "# Select attributes\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22'], \n",
    "                       'incident_angle': ['th0.110']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "# For each selected DA, fit the π-π peak\n",
    "all_outs = {}\n",
    "all_params = {}\n",
    "prev_fit_results = None\n",
    "for DA in tqdm(selected_DAs):\n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "    binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi').compute()\n",
    "    \n",
    "    outs, params = fit_peaks_across_chi_bins(binned_DA, prev_fit_results=prev_fit_results)\n",
    "    prev_fit_results = params\n",
    "    \n",
    "        \n",
    "    # Append to in-memory dictionaries\n",
    "    all_outs[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}'] = outs\n",
    "    all_params[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}'] = params\n",
    "\n",
    "#         # Save paths\n",
    "#         outPath.joinpath('full_linefits_v1').mkdir(exist_ok=True)\n",
    "#         savePath = outPath.joinpath('full_linefits_v1', f'chiWidth-{chi_width}_chiBins-{chi_bins}_chiRange{chi_min}-{chi_max}')\n",
    "#         savePath.mkdir(exist_ok=True)\n",
    "#         sampleFitPath = savePath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}')\n",
    "#         sampleFitPath.mkdir(exist_ok=True)\n",
    "                                          \n",
    "#         # Save plot \n",
    "#         fig.savefig(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_chi{key}_pi-pi_fit.png'), dpi=150)\n",
    "\n",
    "#         plt.show()\n",
    "#         plt.close('all')\n",
    "                         \n",
    "#     # Save fit params:\n",
    "#     json_data = json.dumps(params)\n",
    "#     with open(str(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_params.json')), 'w') as f:\n",
    "#         f.write(json_data)   \n",
    "                                          \n",
    "#     # Save fit results\n",
    "#     for chi_bin, out in outs.items():\n",
    "#         with sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_{chi_bin}_fit_result.txt').open(mode='w') as f:\n",
    "#             f.write(out.fit_report())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5484045-6a66-4e1c-8912-1a05933e497b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_pipi_model(sliced_DA, prev_params=None, pipi_qlims=None):\n",
    "    \"\"\"\n",
    "    Build lmfit model with initial parameter guesses, optionally using previous fit results.\n",
    "    \"\"\"\n",
    "    x = sliced_DA.sel(qr=slice(pipi_qlims[0], pipi_qlims[1])).qr.data\n",
    "    y = sliced_DA.sel(qr=slice(pipi_qlims[0], pipi_qlims[1])).data\n",
    "    \n",
    "    # Define a dictionary for peak models\n",
    "    peak_models = {\n",
    "        'bkg': models.LinearModel(prefix='bkg_'),\n",
    "        'pkgroup': models.PseudoVoigtModel(prefix='pkgroup_'),\n",
    "        'pipi': models.PseudoVoigtModel(prefix='pipi_'),\n",
    "        'pk1p86': models.PseudoVoigtModel(prefix='pk1p86_')\n",
    "    }\n",
    "\n",
    "    # Set up initial parameter guesses (use previous fit parameters if available)\n",
    "    pars = lmfit.Parameters()\n",
    "    \n",
    "    # Add background model\n",
    "    point_y = float(sliced_DA.min()) * 0.8  # Modify the initial y value\n",
    "    pars += peak_models['bkg'].make_params(intercept=point_y, slope=0)\n",
    "    pars['bkg_intercept'].set(vary=False)\n",
    "    pars['bkg_slope'].set(vary=False)\n",
    "\n",
    "    # Add peaks, updating with previous parameters if available\n",
    "    peak_definitions = [\n",
    "        # {'name': 'pkgroup', 'center': (1.4, 1.35, 1.45), 'amplitude_min': 50, 'sigma_min': 0.2, 'sigma_max': 0.5},\n",
    "        {'name': 'pipi', 'center': (1.72, 1.705, 1.735), 'amplitude_min': 0, 'sigma_min': 0.1, 'sigma_max': 0.3},\n",
    "        # {'name': 'pk1p86', 'center': (1.86, 1.84, 1.9), 'amplitude_min': 0, 'amplitude_max': 80, 'sigma_min': 0.2, 'sigma_max': 0.5}\n",
    "        {'name': 'pkgroup', 'center': (1.4, 1.35, 1.45), 'amplitude_min': 0},\n",
    "        # {'name': 'pipi', 'center': (1.72, 1.69, 1.75), 'amplitude_min': 0},\n",
    "        {'name': 'pk1p86', 'center': (1.86, 1.84, 1.9), 'amplitude_min': 0}\n",
    "    ]\n",
    "\n",
    "    for peak in peak_definitions:\n",
    "        model_name = peak['name']\n",
    "        model = peak_models[model_name]\n",
    "        center, center_min, center_max = peak['center']\n",
    "        pars += model.guess(y, x, center=center)\n",
    "        pars[f'{model_name}_center'].set(min=center_min, max=center_max)\n",
    "        pars[f'{model_name}_amplitude'].set(min=peak.get('amplitude_min', 0), max=peak.get('amplitude_max', None))\n",
    "        # pars[f'{model_name}_sigma'].set(min=peak.get('sigma_min', 0), max=peak.get('sigma_max', None))\n",
    "        if prev_params:  # Use previous fit results to update initial guesses\n",
    "            pars[f'{model_name}_center'].set(value=prev_params.get(f'{model_name}_center', center))\n",
    "            pars[f'{model_name}_amplitude'].set(value=prev_params.get(f'{model_name}_amplitude', pars[f'{model_name}_amplitude'].value))\n",
    "\n",
    "    # Combine all models\n",
    "    mod = sum([m for name, m in peak_models.items() if name != 'bkg'], peak_models['bkg'])  # Sum all models starting with the background\n",
    "    return mod, pars\n",
    "\n",
    "def plot_fit_results(out, sliced_DA):\n",
    "    \"\"\"\n",
    "    Function to plot fit results and original data.\n",
    "    \"\"\"\n",
    "    q = sliced_DA.qr.data\n",
    "    I = sliced_DA.data\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set(size_inches=(6.5, 3.5), dpi=120, tight_layout=True)\n",
    "    fig.suptitle(f'Fit: {sn[sliced_DA.sample_ID]} {sliced_DA.sample_pos}, '\n",
    "                 f'alpha={float(sliced_DA.incident_angle[2:])}°, {sliced_DA.chi_bin}° chi', fontsize=12, y=0.93, x=0.53)\n",
    "    \n",
    "    ax.plot(q, I, label='Data', linewidth=2.5)\n",
    "    ax.plot(q, out.best_fit, '--', label='Fit')\n",
    "    \n",
    "    # Plot each component\n",
    "    for key, comp in out.eval_components().items():\n",
    "        ax.plot(q, comp, label=f'{key}')\n",
    "    \n",
    "    ax.set(xlabel='Q [$\\AA^{-1}$]', ylabel='Intensity [arb. units]', yscale='linear')\n",
    "    ax.legend(title=f'$R^2$ = {out.rsquared:.3f}, $\\chi_r$ = {out.redchi:.4f}', loc='upper left', bbox_to_anchor=(1, 1.05))\n",
    "    ax.grid(True, axis='x')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def fit_pipi_across_chi_bins(binned_DA, prev_fit_results=None, pipi_qlims=None):\n",
    "    \"\"\"\n",
    "    Fit peaks across chi bins, updating initial guesses with the previous fit's results.\n",
    "    \"\"\"\n",
    "    outs = {}\n",
    "    params = {}\n",
    "    prev_params = prev_fit_results or {}\n",
    "    \n",
    "    for i, chi_bin in tqdm(enumerate(binned_DA.chi_bins.data), desc='chi_bins', total=len(binned_DA.chi_bins.data)):\n",
    "        key = f'{round(chi_bin.left)}-{round(chi_bin.right)}'\n",
    "        sel_DA = binned_DA.sel(chi_bins=chi_bin)\n",
    "        sel_DA.attrs['chi_bin'] = chi_bin\n",
    "        \n",
    "        # Build model and run fit using previous parameters\n",
    "        model, pars = build_pipi_model(sel_DA, prev_params=prev_params, pipi_qlims=pipi_qlims)\n",
    "        out = model.fit(sel_DA.sel(qr=slice(pipi_qlims[0], pipi_qlims[1])).data, pars, x=sel_DA.sel(qr=slice(pipi_qlims[0], pipi_qlims[1])).qr.data)\n",
    "        outs[key] = out\n",
    "        \n",
    "        # Store parameters for updating the next bin's initial guesses\n",
    "        prev_params = out.params.valuesdict()\n",
    "        params[key] = prev_params\n",
    "        \n",
    "        # Optionally plot\n",
    "        plot_fit_results(out, sel_DA.sel(qr=slice(pipi_qlims[0], pipi_qlims[1])))\n",
    "\n",
    "    return outs, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02fafc-37ee-43b7-bf40-752c117c0046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "# pipi peak fitting for each chi bin\n",
    "\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "chi_min, chi_width, chi_bins = [10, 8, 9]  \n",
    "# chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "q_min = 0.2\n",
    "q_max = 1.86\n",
    "\n",
    "# Select attributes\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22'], \n",
    "                       'incident_angle': ['th0.110']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "# For each selected DA, fit the π-π peak\n",
    "all_outs = {}\n",
    "all_params = {}\n",
    "prev_fit_results = None\n",
    "for DA in tqdm(selected_DAs):\n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "    binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi').compute()\n",
    "    \n",
    "    outs, params = fit_pipi_across_chi_bins(binned_DA, prev_fit_results=prev_fit_results, pipi_qlims=(1.4, 1.86))\n",
    "    prev_fit_results = params\n",
    "    \n",
    "        \n",
    "    # Append to in-memory dictionaries\n",
    "    all_outs[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}'] = outs\n",
    "    all_params[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}'] = params\n",
    "\n",
    "#         # Save paths\n",
    "#         outPath.joinpath('full_linefits_v1').mkdir(exist_ok=True)\n",
    "#         savePath = outPath.joinpath('full_linefits_v1', f'chiWidth-{chi_width}_chiBins-{chi_bins}_chiRange{chi_min}-{chi_max}')\n",
    "#         savePath.mkdir(exist_ok=True)\n",
    "#         sampleFitPath = savePath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}')\n",
    "#         sampleFitPath.mkdir(exist_ok=True)\n",
    "                                          \n",
    "#         # Save plot \n",
    "#         fig.savefig(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_chi{key}_pi-pi_fit.png'), dpi=150)\n",
    "\n",
    "#         plt.show()\n",
    "#         plt.close('all')\n",
    "                         \n",
    "#     # Save fit params:\n",
    "#     json_data = json.dumps(params)\n",
    "#     with open(str(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_params.json')), 'w') as f:\n",
    "#         f.write(json_data)   \n",
    "                                          \n",
    "#     # Save fit results\n",
    "#     for chi_bin, out in outs.items():\n",
    "#         with sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_{chi_bin}_fit_result.txt').open(mode='w') as f:\n",
    "#             f.write(out.fit_report())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea722e-a345-4084-b3c7-c786f4587046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sigmas = np.array([])\n",
    "centers = np.array([])\n",
    "amplitudes = np.array([])\n",
    "\n",
    "for key in params.keys():\n",
    "    # print(params[key]['pkgroup_sigma'])\n",
    "    sigmas = np.append(sigmas, params[key]['pkgroup_sigma'])\n",
    "    centers = np.append(centers, params[key]['pkgroup_center'])\n",
    "    amplitudes = np.append(amplitudes, params[key]['pkgroup_amplitude'])\n",
    "    \n",
    "print(sigmas.mean())\n",
    "print(centers.mean())\n",
    "print(amplitudes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394dbcaf-578b-498e-9d14-7bea7ce0ae38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak = 'pipi'\n",
    "sigmas = np.array([])\n",
    "centers = np.array([])\n",
    "amplitudes = np.array([])\n",
    "\n",
    "for key in params.keys():\n",
    "    sigmas = np.append(sigmas, params[key][f'{peak}_sigma'])\n",
    "    centers = np.append(centers, params[key][f'{peak}_center'])\n",
    "    amplitudes = np.append(amplitudes, params[key][f'{peak}_amplitude'])\n",
    "    \n",
    "print(sigmas.mean())\n",
    "print(centers.mean())\n",
    "print(amplitudes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34fd25-3663-4d0a-a982-1de8019523e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak = 'pk1p86'\n",
    "sigmas = np.array([])\n",
    "centers = np.array([])\n",
    "amplitudes = np.array([])\n",
    "\n",
    "for key in params.keys():\n",
    "    sigmas = np.append(sigmas, params[key][f'{peak}_sigma'])\n",
    "    centers = np.append(centers, params[key][f'{peak}_center'])\n",
    "    amplitudes = np.append(amplitudes, params[key][f'{peak}_amplitude'])\n",
    "    \n",
    "print(sigmas.mean())\n",
    "print(centers.mean())\n",
    "print(amplitudes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b78d8d-8065-47ee-98c6-20ecc44939a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sigmas = np.array([])\n",
    "centers = np.array([])\n",
    "amplitudes = np.array([])\n",
    "\n",
    "for key in params.keys():\n",
    "    # print(params[key]['pkgroup_sigma'])\n",
    "    sigmas = np.append(sigmas, params[key]['pkgroup_sigma'])\n",
    "    centers = np.append(centers, params[key]['pkgroup_center'])\n",
    "    amplitudes = np.append(amplitudes, params[key]['pkgroup_amplitude'])\n",
    "    \n",
    "print(sigmas.mean())\n",
    "print(centers.mean())\n",
    "print(amplitudes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149908f9-4454-4564-aa4e-a5f87d9a97e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sigmas.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937b058-4458-45c3-a72e-054535632160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "# Pipi peak fitting for each chi bin\n",
    "\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "q_min = 0.2\n",
    "q_max = 1.86\n",
    "\n",
    "# Select attributes\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22'], \n",
    "                       'incident_angle': ['th0.110']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "# For each selected DA, fit the π-π peak\n",
    "all_outs = {}\n",
    "all_params = {}\n",
    "prev_fit_results = None\n",
    "for DA in tqdm(selected_DAs):\n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "    binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi').compute()\n",
    "    \n",
    "    outs, params = fit_pipi_across_chi_bins(binned_DA, prev_fit_results=prev_fit_results, pipi_qlims=)\n",
    "    prev_fit_results = params\n",
    "    \n",
    "        \n",
    "    # Append to in-memory dictionaries\n",
    "    all_outs[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}'] = outs\n",
    "    all_params[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}'] = params\n",
    "\n",
    "#         # Save paths\n",
    "#         outPath.joinpath('full_linefits_v1').mkdir(exist_ok=True)\n",
    "#         savePath = outPath.joinpath('full_linefits_v1', f'chiWidth-{chi_width}_chiBins-{chi_bins}_chiRange{chi_min}-{chi_max}')\n",
    "#         savePath.mkdir(exist_ok=True)\n",
    "#         sampleFitPath = savePath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}')\n",
    "#         sampleFitPath.mkdir(exist_ok=True)\n",
    "                                          \n",
    "#         # Save plot \n",
    "#         fig.savefig(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_chi{key}_pi-pi_fit.png'), dpi=150)\n",
    "\n",
    "#         plt.show()\n",
    "#         plt.close('all')\n",
    "                         \n",
    "#     # Save fit params:\n",
    "#     json_data = json.dumps(params)\n",
    "#     with open(str(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_params.json')), 'w') as f:\n",
    "#         f.write(json_data)   \n",
    "                                          \n",
    "#     # Save fit results\n",
    "#     for chi_bin, out in outs.items():\n",
    "#         with sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_{chi_bin}_fit_result.txt').open(mode='w') as f:\n",
    "#             f.write(out.fit_report())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9edaee-ff6f-4d3a-b747-f22bc931f213",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### New fiting code (inspired from Tom's notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14a5b1-250b-412e-97aa-3b4222b89360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lmfit import Model, Parameters\n",
    "from lmfit.model import save_modelresult, load_modelresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb4402-dda9-4d0d-9751-5bd91089a4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define generic functions for model:\n",
    "def linear(x, m, b):\n",
    "    \"\"\"\n",
    "    1-degree polynomial (straight line)\n",
    "    \n",
    "    e.g.: f(x) = mx + b\n",
    "    \n",
    "    Inputs:\n",
    "        x: array-like, independent variable for function\n",
    "        m: slope \n",
    "        b: y intercept value\n",
    "    \"\"\"\n",
    "    return m*x + b \n",
    "\n",
    "def exp(x, A, d):\n",
    "    \"\"\"\n",
    "    Exponential decay function: f(x) = A * exp(-d * x)\n",
    "    \n",
    "    Inputs:\n",
    "        x: array-like, independent variable for function\n",
    "        A: initial amplitude (value at x=0)\n",
    "        d: decay value (rate value, larger values lead to faster decay)\n",
    "    \"\"\"\n",
    "    return A * np.exp(-d * x)\n",
    "\n",
    "def lorentz_peak(x, A, sigma, x0):\n",
    "    \"\"\"\n",
    "    Lorentzian peak function\n",
    "\n",
    "    Parameters:\n",
    "    x : array_like, The independent variable where the Lorentz peak is evaluated.\n",
    "    A : float, Amplitude of the Lorentzian peak (integrated scattering intensity).\n",
    "    sigma : float, Half-width at half-maximum (HWHM) of the Lorentzian peak.\n",
    "    x0 : float, Position of the center of the Lorentzian peak.\n",
    "\n",
    "    Returns:\n",
    "    array_like, Values of the Lorentzian function at each x.\n",
    "    \"\"\"\n",
    "    return (A / np.pi) * (sigma / ((x - x0)**2 + sigma**2))\n",
    "\n",
    "def gaussian_peak(x, A, sigma, x0):\n",
    "    \"\"\"\n",
    "    Gaussian peak function\n",
    "    \n",
    "    Parameters:\n",
    "    \"\"\"\n",
    "    return (A / (sigma * np.sqrt(2 * np.pi))) * np.exp((x - x0)**2 / (2 * sigma**2))\n",
    "\n",
    "def pseudovoigt(x, A, sigma, x0, fraction):\n",
    "    \"\"\"\n",
    "    Pseudo-Voigt peak function\n",
    "    \"\"\"\n",
    "    return (fraction * gaussian_peak(x, A, sigma, x0)) + ((1 - fraction) * lorentz_peak(x, A, sigma, x0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4c7ec-839a-4064-96ee-54841531d844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define full PM6 GIWAXS model function:\n",
    "def PM6_model_custom(x, m, b, exp_A, d, \n",
    "              amp1, sigma1, center1, fraction1,\n",
    "              amp2, sigma2, center2, fraction2,\n",
    "              amp3, sigma3, center3, fraction3,\n",
    "              amp4, sigma4, center4, fraction4,\n",
    "              amp5, sigma5, center5, fraction5,\n",
    "              amp6, sigma6, center6, fraction6):\n",
    "    \"\"\"\n",
    "    Combines: 1 linear background, 1 exponential decay background, and 6 Pseudo-Voigt peaks\n",
    "    \"\"\"\n",
    "    \n",
    "    lin_bkg = linear(x, m, b)\n",
    "    exp_bkg = exp(x, exp_A, d)\n",
    "    \n",
    "    lamella =  pseudovoigt(x, amp1, sigma1, center1, fraction1)\n",
    "    backbone = pseudovoigt(x, amp2, sigma2, center2, fraction2)\n",
    "    pk0p9 =    pseudovoigt(x, amp3, sigma3, center3, fraction3)\n",
    "    pk1p2 =    pseudovoigt(x, amp4, sigma4, center4, fraction4)\n",
    "    pkalkyl =  pseudovoigt(x, amp5, sigma5, center5, fraction5)\n",
    "    pipi =     pseudovoigt(x, amp6, sigma6, center6, fraction6)\n",
    "    \n",
    "    return lin_bkg + exp_bkg + lamella + backbone + pk0p9 + pk1p2 + pkalkyl + pipi\n",
    "\n",
    "def PM6_model_lmfit(x, m, b, exp_A, d, \n",
    "              amp1, sigma1, center1, fraction1,\n",
    "              amp2, sigma2, center2, fraction2,\n",
    "              amp3, sigma3, center3, fraction3,\n",
    "              amp4, sigma4, center4, fraction4,\n",
    "              amp5, sigma5, center5, fraction5,\n",
    "              amp6, sigma6, center6, fraction6):\n",
    "    \"\"\"\n",
    "    Combines: 1 linear background, 1 exponential decay background, and 6 Pseudo-Voigt peaks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define all models to include in fitting\n",
    "    bkg_mod = models.LinearModel(prefix='bkg_')\n",
    "    pars = bkg_mod.make_params(intercept=b, slope=m)\n",
    "    pars['bkg_intercept'].set(vary=False)\n",
    "    pars['bkg_slope'].set(vary=False)\n",
    "    \n",
    "    exp_mod = models.ExponentialModel(prefix='exp_')\n",
    "    pars += exp_mod.make_params(decay=d, amplitude=exp_A)\n",
    "    pars['exp_decay'].set(min=0.01)\n",
    "    pars['exp_amplitude'].set(min=0.1)\n",
    "    \n",
    "    lamella_mod = models.PseudoVoigtModel(prefix='lamella_')  # lamella peak at 0.32\n",
    "    pars += lamella_mod.make_params(center={'value':0.31, 'min':0.28, 'max':0.34}, \n",
    "                                    sigma ={'value':0.09/2, 'min':0.01/2, 'max':0.2/2})\n",
    "    pars['lamella_amplitude'].set(min=30)\n",
    "    \n",
    "    backbone_mod = models.PseudoVoigtModel(prefix='backbone_')  # backbone peak at 0.66\n",
    "    pars += backbone_mod.guess(y, x, center=0.66)\n",
    "    pars['backbone_amplitude'].set(min=0, max=10)\n",
    "    pars['backbone_center'].set(min=0.63, max=0.73)\n",
    "    # pars['backbone_sigma'].set(max=0.1)\n",
    "\n",
    "    pk0p9_mod = models.PseudoVoigtModel(prefix='pk0p9_')  # 300  peak around 0.9\n",
    "    pars += pk0p9_mod.guess(y, x, center=0.93)\n",
    "    pars['pk0p9_amplitude'].set(min=0)\n",
    "    pars['pk0p9_center'].set(min=0.88, max=0.99)\n",
    "\n",
    "    pk1p2_mod = models.PseudoVoigtModel(prefix='pk1p2_')  # 021? peak around 1.2\n",
    "    pars += pk1p2_mod.guess(y, x, center=1.2)\n",
    "    # pars += pk1p2_mod.make_params(center={'value':1.2, 'min':1.15, 'max':1.28}, \n",
    "    #                               sigma ={'value':0.8/2, 'min':0.01/2, 'max':3/2})\n",
    "    pars['pk1p2_amplitude'].set(min=0)\n",
    "    pars['pk1p2_center'].set(min=1.18, max=1.26)\n",
    "    \n",
    "    pkgroup_mod = models.PseudoVoigtModel(prefix='pkgroup_')  # broad collection of slip-stack peaks around 1.2-2\n",
    "    pars += pkgroup_mod.guess(y, x, center=1.4)\n",
    "    # pars += pkgroup_mod.make_params(center={'value':1.4, 'min':1.3, 'max':1.6}, \n",
    "    #                                 sigma ={'value':0.8/2, 'min':0.01/2, 'max':3/2})\n",
    "    pars['pkgroup_center'].set(min=1.3, max=1.5)\n",
    "    pars['pkgroup_amplitude'].set(min=10)\n",
    "    pars['pkgroup_sigma'].set(min=0.1, max=0.4)\n",
    "\n",
    "    pipi_mod = models.PseudoVoigtModel(prefix='pipi_')  # pi-pi peak! around 1.73\n",
    "    pars += pipi_mod.guess(y, x, center=1.73)\n",
    "    pars['pipi_amplitude'].set(min=0, max=500)\n",
    "    pars['pipi_center'].set(min=1.7, max=1.76)\n",
    "    pars['pipi_sigma'].set(max=0.3)\n",
    "    \n",
    "    pk1p86_mod = models.PseudoVoigtModel(prefix='pk1p86_')  # highq peak\n",
    "    pars += pk1p86_mod.guess(y, x, center=1.86)\n",
    "    pars['pk1p86_amplitude'].set(min=0, max=80)\n",
    "    pars['pk1p86_center'].set(min=1.84, max=1.9)\n",
    "\n",
    "    # Combine into full model\n",
    "    mod = bkg_mod + exp_mod + lamella_mod + backbone_mod + pk0p9_mod + pk1p2_mod + pkgroup_mod + pipi_mod + pk1p86_mod\n",
    "    \n",
    "    return lin_bkg + exp_bkg + lamella + backbone + pk0p9 + pk1p2 + pkalkyl + pipi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40c99c-6a6c-4748-8255-aca94a9c8b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initalize lmfit model and set default parameter values\n",
    "model = Model(PM6_model)\n",
    "\n",
    "# Create a Parameters object and set initial values and constraints\n",
    "# variables must match arguments for fitting function\n",
    "# variables are defined as list ['variable_name', start_value, lower_bound, upper_bound, Allow to vary?(True/False)]\n",
    "m = ['m', 0, 0, np.inf, False]\n",
    "b = ['b', 0, 0, np.inf, False]  # will likely need to revise later to be based on a diffuse value in the data\n",
    "exp_A = ['exp_A', 2000, 0, 5000, True]\n",
    "d = ['d', 1, 0.01, 100, True]\n",
    "\n",
    "amp1 = ['amp1', 1000, 50, 1e3, True]  # lamella\n",
    "amp2 = ['amp2', 50, 0, 1e3, True]  # backbone\n",
    "amp3 = ['amp3', 10, 0, 1e3, True]  # pk0p9\n",
    "amp4 = ['amp4', 5, 0, 1e3, True]  # pk1p2\n",
    "amp5 = ['amp5', 40, 0, 1e3, True]  # pkalkyl\n",
    "amp6 = ['amp6', 30, 0, 1e3, True]  # pipi\n",
    "\n",
    "sigma1 = ['sigma1', 0.05, 0.01, 0.5, True]  # lamella\n",
    "sigma2 = ['sigma2', 0.01, 0.005, 0.5, True]  # backbone\n",
    "sigma3 = ['sigma3', 0.1, 0.01, 0.5, True]  # pk0p9\n",
    "sigma4 = ['sigma4', 0.1, 0.01, 0.5, True]  # pk1p2\n",
    "sigma5 = ['sigma5', 0.3, 0.01, 0.5, True]  # pkalkyl\n",
    "sigma6 = ['sigma6', 0.1, 0.01, 0.5, True]  # pipi\n",
    "\n",
    "center1 = ['center1', 0.31, 0.26, 0.35, True]  # lamella\n",
    "center2 = ['center2', 0.66, 0.63, 0.73, True]  # backbone\n",
    "center3 = ['center3', 0.93, 0.88, 0.99, True]  # pk0p9\n",
    "center4 = ['center4', 1.20, 1.18, 1.22, True]  # pk1p2\n",
    "center5 = ['center5', 1.40, 1.35, 1.45, True]  # pkalkyl\n",
    "center6 = ['center6', 1.73, 1.70, 1.76, True]  # pipi\n",
    "\n",
    "fraction1 = ['fraction1', 0.5, 0, 1, True]  # lamella\n",
    "fraction2 = ['fraction2', 0.5, 0, 1, True]  # backbone\n",
    "fraction3 = ['fraction3', 0.5, 0, 1, True]  # pk0p9\n",
    "fraction4 = ['fraction4', 0.5, 0, 1, True]  # pk1p2\n",
    "fraction5 = ['fraction5', 0.5, 0, 1, True]  # pkalkyl\n",
    "fraction6 = ['fraction6', 0.5, 0, 1, True]  # pipi\n",
    "\n",
    "# List all above variables (maybe this should just be a dictionary with the name of each being the key?)\n",
    "variable_list = [m, b, exp_A, d, \n",
    "                 amp1, amp2, amp3, amp4, amp5, amp6,\n",
    "                 sigma1, sigma2, sigma3, sigma4, sigma5, sigma6,\n",
    "                 center1, center2, center3, center4, center5, center6,\n",
    "                 fraction1, fraction2, fraction3, fraction4, fraction5, fraction6]\n",
    "\n",
    "# Create lmfit parameters:\n",
    "params = Parameters()\n",
    "for var in variable_list:\n",
    "    params.add(var[0], value=var[1], min=var[2], max=var[3], vary=var[4])\n",
    "    \n",
    "# Define weighting if desired (or set to None):\n",
    "weights = None\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd1f111-8454-4e91-a734-374a342de537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply model + params to fit GIWAXS data IvsQ line cuts (binned/meaned along chi):\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "# Full fit for each chi bin\n",
    "\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "# chi_min, chi_width, chi_bins = [10, 8, 9]  \n",
    "chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "# Set q limits\n",
    "q_min = 0.2\n",
    "q_max = 1.82\n",
    "\n",
    "# Select attributes\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22'], \n",
    "                       'incident_angle': ['th0.110']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "# For each selected DA, fit the π-π peak\n",
    "for DA in tqdm(selected_DAs):\n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "    binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi').compute()\n",
    "    \n",
    "    for i, chi_bin in tqdm(enumerate(binned_DA.chi_bins.data), desc='chi_bins', total=len(binned_DA.chi_bins.data)):\n",
    "        key = f'{str(round(chi_bin.left))}-{str(round(chi_bin.right))}'\n",
    "        sel_DA = binned_DA.sel(chi_bins=chi_bin)\n",
    "        sel_DA.attrs['chi_bin'] = chi_bin\n",
    "        \n",
    "        x = sel_DA.qr.data\n",
    "        y = sel_DA.data\n",
    "        # result = model.fit(data=y, params=params, weights=weights, x=x, method='leastsq')\n",
    "        result = model.fit(y, params=params, x=x, nan_policy='omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8131d-b1c8-48f3-92cf-c669aeb4f7df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934cc6f-e4c3-426a-a8f1-ec665346021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "# Peak fitting for each chi bin\n",
    "\n",
    "# Set chi bounds & bins, q bounds, etc.\n",
    "# chi_min, chi_width, chi_bins = [10, 24, 3] \n",
    "# chi_min, chi_width, chi_bins = [10, 12, 6]  \n",
    "chi_min, chi_width, chi_bins = [10, 4, 18]  \n",
    "chi_max = chi_min + (chi_width * chi_bins)\n",
    "colors = plt.cm.viridis_r(np.linspace(0.15,1,chi_bins))\n",
    "\n",
    "q_min = 0.2\n",
    "q_max = 1.86\n",
    "\n",
    "# Select attributes\n",
    "# selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26', \n",
    "#                                      'AL31', 'AL41', 'AL28', 'AL38']}\n",
    "selected_attrs_dict = {'sample_ID': ['AL22', 'AL36', 'AL25', 'AL26',\n",
    "                                     'AL31', 'AL41', 'AL28', 'AL38'], \n",
    "                       'incident_angle': ['th0.080']}\n",
    "# selected_attrs_dict = {}\n",
    "selected_DAs = select_attrs(folded_corr_DAs, selected_attrs_dict)\n",
    "\n",
    "# For each selected DA, fit the π-π peak\n",
    "all_outs = {}\n",
    "all_params = {}\n",
    "for DA in tqdm(selected_DAs):\n",
    "    sliced_DA = DA.sel(chi=slice(chi_min,chi_max), qr=slice(q_min,q_max))\n",
    "    binned_DA = sliced_DA.groupby_bins('chi', chi_bins).sum('chi').compute()\n",
    "    outs = {}\n",
    "    params = {}\n",
    "    for i, chi_bin in tqdm(enumerate(binned_DA.chi_bins.data[::1]), desc='chi_bins', total=len(binned_DA.chi_bins.data)):\n",
    "        key = f'{str(round(chi_bin.left))}-{str(round(chi_bin.right))}'\n",
    "        sel_DA = binned_DA.sel(chi_bins=chi_bin)\n",
    "        sel_DA.attrs['chi_bin'] = chi_bin\n",
    "        outs[key], fig, ax = full_fit(sel_DA)\n",
    "        params[key] = outs[key].params.valuesdict()\n",
    "\n",
    "        # Save paths\n",
    "        outPath.joinpath('full_linefits_v1').mkdir(exist_ok=True)\n",
    "        savePath = outPath.joinpath('full_linefits_v1', f'chiWidth-{chi_width}_chiBins-{chi_bins}_chiRange{chi_min}-{chi_max}')\n",
    "        savePath.mkdir(exist_ok=True)\n",
    "        sampleFitPath = savePath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}')\n",
    "        sampleFitPath.mkdir(exist_ok=True)\n",
    "                                          \n",
    "        # Save plot \n",
    "        fig.savefig(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_chi{key}_pi-pi_fit.png'), dpi=150)\n",
    "\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "                         \n",
    "    # Save fit params:\n",
    "    json_data = json.dumps(params)\n",
    "    with open(str(sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_params.json')), 'w') as f:\n",
    "        f.write(json_data)   \n",
    "                                          \n",
    "    # Save fit results\n",
    "    for chi_bin, out in outs.items():\n",
    "        with sampleFitPath.joinpath(f'{sn_id[DA.sample_ID]}_{DA.sample_pos}_{DA.incident_angle}_{chi_bin}_fit_result.txt').open(mode='w') as f:\n",
    "            f.write(out.fit_report())\n",
    "        \n",
    "    # Append to full in-memory dictionaries\n",
    "    all_outs[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}'] = outs\n",
    "    all_params[f'{sn_id[DA.sample_ID]}_{DA.sample_pos}'] = params\n",
    "\n",
    "# # Save model_params.json in save folder for all samples\n",
    "# json_data = json.dumps(all_params)\n",
    "# with open(str(savePath.joinpath('model_params.json')), 'w') as f:\n",
    "#     f.write(json_data)\n",
    "\n",
    "# # Create full fit reports folder and save entire report .txt files inside:\n",
    "# savePath.joinpath('fit_reports').mkdir(exist_ok=True)\n",
    "# for film, outs in all_outs.items():\n",
    "#     for chi_bin, out in outs.items():\n",
    "#         with savePath.joinpath('fit_reports', f'{film}_{chi_bin}_fit_result.txt').open(mode='w') as f:\n",
    "#             f.write(out.fit_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd4068-986d-4a1b-9993-6562fde75222",
   "metadata": {},
   "source": [
    "### Load saved fit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017df05-ac54-4922-a4c2-be0647f25c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitsPath = pathlib.Path('/nsls2/data/cms/proposals/2023-3/pass-311415/AL_2024C2/processed_data/full_linefits_v1/chiWidth-4_chiBins-18_chiRange10-82/')\n",
    "[f.name for f in fitsPath.glob('PM6*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e081723-a91e-49df-89cc-a2e59f638a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Combine json files, thanks GPT:\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# json1 = '/nsls2/data/cms/proposals/2023-3/pass-311415/AL_2024C2/processed_data/full_linefits_v1/chiWidth-4_chiBins-18_chiRange10-82/model_params.json'\n",
    "# json2 = '/nsls2/data/cms/proposals/2023-3/pass-311415/AL_2024C2/processed_data/full_linefits_v1/chiWidth-4_chiBins-18_chiRange10-82/model_params_5CN.json'\n",
    "\n",
    "# # Load the two JSON files\n",
    "# with open(json1, 'r') as f1:\n",
    "#     json_data_1 = json.load(f1)\n",
    "\n",
    "# with open(json2, 'r') as f2:\n",
    "#     json_data_2 = json.load(f2)\n",
    "\n",
    "# # Merge the dictionaries\n",
    "# all_params = {**json_data_1, **json_data_2}\n",
    "\n",
    "# # Save each sample as an individual JSON file\n",
    "# for sample_name, sample_data in all_params.items():\n",
    "#     samplePath = fitsPath.joinpath(sample_name)\n",
    "#     savePath = samplePath.joinpath(f'{sample_name}_params.json')\n",
    "#     with open(savePath, 'w') as sample_file:\n",
    "#         json.dump(sample_data, sample_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa9321-3f46-4f8e-bc0b-881c1de97ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load jsons into one dict, from each filepath\n",
    "all_params = {}\n",
    "for sampleFitPath in fitsPath.glob('PM6*th0.110*'):\n",
    "    sample_name = sampleFitPath.name\n",
    "    jsonPath = sampleFitPath.joinpath(f'{sample_name}_params.json')\n",
    "    with open(jsonPath, 'r') as json_file:\n",
    "        params = json.load(json_file)\n",
    "    all_params[sample_name] = params\n",
    "    \n",
    "all_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4b3d1-2f6d-4e99-820c-2e6519a98a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_params['PM6_1CN-CF_x0.000_th0.110']['10-14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6e5a5-119c-486c-9301-b7c8657ee086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot pseudovoigt fractions, d-spacings, scherrer coherence lengths, and normalized peak areas vs chi\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "\n",
    "# Initialize figures to be populated later\n",
    "fig_psvoigt, ax_psvoigt = plt.subplots(figsize=((5.5,2.5)), dpi=150, tight_layout=True)\n",
    "fig_dspacing, ax_dspacing = plt.subplots(figsize=((5.5,2.5)), dpi=150, tight_layout=True)\n",
    "fig_ccl, ax_ccl = plt.subplots(figsize=((5.5,2.5)), dpi=150, tight_layout=True)\n",
    "fig_pole, ax_pole = plt.subplots(figsize=((5.5,2.5)), dpi=150, tight_layout=True)\n",
    "# fig_pole, ax_pole = plt.subplots(figsize=((2.8,3.3)), dpi=150, tight_layout=True)\n",
    "\n",
    "# Choose sample sets (by folder/sample name):\n",
    "peak = 'lamella'\n",
    "incident_angle = 'th0.110'\n",
    "PM6_CF_samples = [f'PM6_0CN-CF_x0.000_{incident_angle}', f'PM6_p5CN-CF_x0.000_{incident_angle}', f'PM6_1CN-CF_x2.000_{incident_angle}', f'PM6_5CN-CF_x2.000_{incident_angle}']\n",
    "PM6_CB_samples = [f'PM6_0CN-CB_x0.000_{incident_angle}', f'PM6_p5CN-CB_x0.000_{incident_angle}', f'PM6_1CN-CB_x0.000_{incident_angle}', f'PM6_5CN-CB_x0.000_{incident_angle}']\n",
    "\n",
    "\n",
    "# Define custom colors for each film, here I've chosen just two shades along the same sequention colorbar for each sample group\n",
    "colors = plt.cm.viridis_r(np.linspace(0,1,len(PM6_CF_samples)))\n",
    "\n",
    "### Extracting info from all_params & populating ___ vs chi plots:\n",
    "summed_peak_areas = {}\n",
    "avg_dspacings = {}\n",
    "avg_peak_centers = {}\n",
    "avg_coherence_lengths = {}\n",
    "# for i, film in enumerate(all_params.keys()):\n",
    "for i, film in enumerate(PM6_CF_samples):\n",
    "# for i, film in enumerate(PM6_CB_samples):\n",
    "    peak_areas = []\n",
    "    pvoigt_fracs = []\n",
    "    peak_centers = []\n",
    "    peak_fwhms = []\n",
    "    \n",
    "    chis = []\n",
    "    for chi_bin, params in all_params[film].items():   \n",
    "        # Get starting value of chi bin\n",
    "        chi = int(chi_bin.split('-')[0]) + 2  # set to midpoint of values\n",
    "        chis.append(chi)\n",
    "        # Get peak areas\n",
    "        peak_area = params[f'{peak}_amplitude']\n",
    "        peak_areas.append(peak_area)\n",
    "        # Get pseudovoigt fraction\n",
    "        pvoigt_frac = params[f'{peak}_fraction']\n",
    "        pvoigt_fracs.append(pvoigt_frac)\n",
    "        # Get peak centers\n",
    "        peak_center = params[f'{peak}_center']\n",
    "        peak_centers.append(peak_center)\n",
    "        # Get peak fwhms\n",
    "        peak_fwhm = params[f'{peak}_fwhm']\n",
    "        peak_fwhms.append(peak_fwhm)\n",
    "\n",
    "    # Get normalized peak areas (divide by sum of all peak areas; adds to 1) and write area sum to dict\n",
    "    peak_areas = np.array(peak_areas)\n",
    "    summed_peak_areas[film] = np.sum(peak_areas)  # Write sum of peak area out, this corresponds to relative extent of crystallinity if thickness normalized\n",
    "    normed_peak_areas = peak_areas / np.sum(peak_areas)\n",
    "\n",
    "    # Calculate d-spacings from peak centers, calculate peak-area-weighted average of d-spacing\n",
    "    dspacings = (2*np.pi) / np.array(peak_centers) \n",
    "    avg_peak_centers[film] = peak_center\n",
    "    avg_dspacing = np.round(np.sum(dspacings*normed_peak_areas), 2)\n",
    "    avg_dspacings[film] = avg_dspacing  \n",
    "\n",
    "    # Calculate coherence length from peak fwhm, calculate peak-area-weighted average of coherence length    \n",
    "    coherence_lengths = (2*np.pi*0.9) / np.array(peak_fwhms) \n",
    "    avg_coherence_length = np.round(np.sum(coherence_lengths*normed_peak_areas), 2)\n",
    "    avg_coherence_lengths[film] = avg_coherence_length\n",
    "\n",
    "    ## Plotting\n",
    "    # Plot pseudovoigt fraction vs chi for each film\n",
    "    ax_psvoigt.errorbar(chis, pvoigt_fracs, label=film, color=colors[i], marker='o')\n",
    "    ax_psvoigt.set(title=f'{peak} pseudo-voigt lorentzian-to-gaussian ratio versus $\\\\chi$',\n",
    "                   ylabel='Lorentzian peak fraction',\n",
    "                   xlabel='Binned $\\\\chi$ value [°]')\n",
    "    ax_psvoigt.legend(title='Film', loc='upper left', bbox_to_anchor=(1,1))\n",
    "\n",
    "    # Plot d-spacing vs chi for each film\n",
    "    ax_dspacing.errorbar(chis, dspacings, label=film, color=colors[i], marker='o', \n",
    "                         capsize=4)\n",
    "    ax_dspacing.set(title=f'{peak} d-spacing versus $\\\\chi$',\n",
    "                    ylabel='d-spacing [Å]',\n",
    "                    xlabel='$\\\\chi$ value [°]')\n",
    "                    # ylim=(3.55,3.8))\n",
    "    ax_dspacing.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.85))\n",
    "    ax_dspacing.grid(visible=True,which='major',axis='y')\n",
    "\n",
    "    # Plot coherence length vs chi for each film\n",
    "    ax_ccl.errorbar(chis, coherence_lengths, label=film, color=colors[i], marker='o', \n",
    "                    capsize=4)\n",
    "    ax_ccl.set(title=f'{peak} crystalline coherence length (CCL) versus $\\\\chi$',\n",
    "               ylabel='CCL [Å]',\n",
    "               xlabel='$\\\\chi$ value [°]')\n",
    "               # ylim=(13,18.5))\n",
    "    ax_ccl.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.85))\n",
    "    ax_ccl.grid(visible=True,which='major',axis='y')\n",
    "    \n",
    "    # Plot 'pole figure' for each film\n",
    "    ax_pole.errorbar(chis, normed_peak_areas, label=film, color=colors[i], marker='o',\n",
    "                     capsize=4)\n",
    "    ax_pole.set(title=f'{peak} pole figure',\n",
    "                ylabel='Peak area [arb. units]',\n",
    "                xlabel='$\\\\chi$ value [°]')\n",
    "    # ax_pole.set_ybound(lower=0, upper=0.21)\n",
    "    ax_pole.grid(visible=True,which='major',axis='y')\n",
    "    # ax_pole.legend(title='Film')    \n",
    "    ax_pole.legend(title='Film', loc='upper left', bbox_to_anchor=(1,0.85))\n",
    "\n",
    "    # Extract chi values for file saving purposes\n",
    "    chi_width = chis[0]-chis[1]\n",
    "    chi_bins = len(chis)\n",
    "    chi_min = chis[-1] - 1\n",
    "    chi_max = chis[0]+chi_width - 1\n",
    "\n",
    "    # # Save\n",
    "    # parent_folder = 'full_chi_fit_results_plots_v1'\n",
    "    # outPath.joinpath(parent_folder).mkdir(exist_ok=True)\n",
    "    # savePath = outPath.joinpath(parent_folder, f'chiWidth-{chi_width}_chiBins-{chi_bins}_chiRange{chi_min}-{chi_max}')\n",
    "    # savePath.mkdir(exist_ok=True)\n",
    "    # fig_psvoigt.savefig(savePath.joinpath(f'pseudovoigt_ratios_pi-pi_fit.png'), dpi=150)\n",
    "    # fig_dspacing.savefig(savePath.joinpath(f'dspacings_pi-pi_fit.png'), dpi=150)\n",
    "    # fig_ccl.savefig(savePath.joinpath(f'coherence_lengths_pi-pi_fit.png'), dpi=150)\n",
    "    # fig_pole.savefig(savePath.joinpath(f'peak_areas_pi-pi_fit.png'), dpi=150)\n",
    "    # # fig_pole.savefig(savePath.joinpath(f'peak_areas_pi-pi_fit_narrow.png'), dpi=150)\n",
    "    \n",
    "plt.show()\n",
    "plt.close('all')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d3c63-afa7-4b5f-a7dd-23d70b4a784b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (current)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
